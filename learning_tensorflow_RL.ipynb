{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL in tensorflow\n",
    "Some experiments to learn how to use tensorflow to do RL.\n",
    "\n",
    "We start with this demo: https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.common = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n #.shape[0] for continuous\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn this into a tf.numpy_function(Tout=[tf.float32, tf.float32, tf.int32])\n",
    "@tf.numpy_function(Tout=[tf.float32, tf.int32, tf.int32])\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Returns, state, reward and done flag given an action\"\"\"\n",
    "\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    return (state.astype(np.float32),\n",
    "            np.array(reward, np.int32),\n",
    "            np.array(done, np.int32))\n",
    "\n",
    "def run_episode(\n",
    "    initial_state: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    max_steps: int\n",
    ") -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Runs a single episode to collect training data\"\"\"\n",
    "\n",
    "    # this is a set of tf tensors to store results\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "        # convert state into batched tensor (batch_size =1)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "\n",
    "        # run the model and get action probs, critic val\n",
    "        action_logits_t, value = model(state)\n",
    "        \n",
    "\n",
    "        # sample next action from action dist\n",
    "        ## need to update this for continuous\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0,0]\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "        # store critic values\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "        # store log probs of chosen action- I think this works because action is discrete for cart pole\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "\n",
    "        # apply action to the environment\n",
    "        state, reward, done = env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "\n",
    "        # store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "\n",
    "        return action_probs, values, rewards\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor,\n",
    "    gamma: float,\n",
    "    standardize: bool = True\n",
    "):\n",
    "    \"\"\"Compute expected returns\"\"\"\n",
    "\n",
    "    n = tf.shape(rewards)[0] \n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    # start at last reward and then accumulate reward sums into returns array\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0,0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum # discounted_sum= 0 for last reward (i.e. first element in loop)\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i, discounted_sum)\n",
    "    returns = returns.stack()[::-1] # take final element\n",
    "\n",
    "    if standardize:\n",
    "        returns = ((returns - tf.reduce_mean(returns)) / \n",
    "                    (tf.reduce_std(returns) + eps))\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,\n",
    "    values: tf.Tensor,\n",
    "    returns: tf.Tensor\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Computes combined actor-critic loss\"\"\"\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    action_log_probs = tf.log(action_probs)\n",
    "    actor_loss = -tf.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# set this as a tf.function for use\n",
    "@tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    optimizer: tf.keras.optimizers.Optimizer,\n",
    "    gamma: float,\n",
    "    max_steps_per_episode: int) -> tf.Tensor:\n",
    "    \"\"\"Runs a model traning step\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # run one episode:\n",
    "        action_probs, values, rewards = run_episode(\n",
    "            initial_state, model, max_steps_per_episode\n",
    "        )\n",
    "\n",
    "        # calculate the expected returns:\n",
    "        returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "        # convert training data to appropriate TF tensor shapes\n",
    "        action_probs, values, returns = [\n",
    "            tf.expand_dims(x, 1) for x in [action_probs, values, returns]\n",
    "        ]\n",
    "\n",
    "        # calculate the loss values\n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "    # compute the gradients from the loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # apply gradients to model params\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    episode_reward = tf.reduce_sum(rewards)\n",
    "\n",
    "    return episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes=10000\n",
    "max_steps_per_episode=500\n",
    "\n",
    "reward_threshold=475\n",
    "running_reward=0\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "t = tqdm.trange(max_episodes)\n",
    "for i in t:\n",
    "    initial_state, info = env.reset()\n",
    "    initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
    "    episode_reward = int(\n",
    "        train_step(initial_state, model, optimizer, gamma, max_steps_per_episode)\n",
    "        )\n",
    "\n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward\n",
    "    )\n",
    "\n",
    "    # show the average episode reward ever 10 episodes\n",
    "    if i % 10 == 0:\n",
    "        print(f'Episode {i}: average reward: {avg_reward}')\n",
    "    \n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
    "        break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
