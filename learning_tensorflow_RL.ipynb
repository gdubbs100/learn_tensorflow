{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL in tensorflow\n",
    "Some experiments to learn how to use tensorflow to do RL.\n",
    "\n",
    "We start with this demo: https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try this guy: https://github.com/philtabor/Actor-Critic-Methods-Paper-To-Code/blob/master/ActorCritic/tf2/agent.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.common = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.actor = layers.Dense(num_actions, activation = 'softmax')\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        gamma: float, \n",
    "        model: tf.keras.Model, \n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        summary_writer):\n",
    "\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.memory = {\n",
    "                'state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'next_state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'action_probs': tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'values' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'rewards' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'done' : tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        }\n",
    "        self.loss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer)\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "    \n",
    "    def _get_action(self, state):\n",
    "\n",
    "        # state = tf.expand_dims(tf.constant(state, tf.float32), 0)\n",
    "        action_probs, value = self.model(\n",
    "                tf.expand_dims(tf.constant(state, tf.float32), 0)\n",
    "            )\n",
    "        action = tfp.distributions.Categorical(probs = action_probs).sample()[0]\n",
    "\n",
    "        return action, action_probs, value\n",
    "\n",
    "    \n",
    "    def get_action(self, state, step):\n",
    "\n",
    "        # run the model\n",
    "        action, action_probs, value = self._get_action(state)\n",
    "        # print(action)\n",
    "\n",
    "        # record outputs - check if need .mark_used()??\n",
    "        # self.memory['action_probs'].write(step, action_probs[0, action]).mark_used()\n",
    "        # self.memory['values'].write(step, value).mark_used()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def log(self, state, next_state, reward, done, step):\n",
    "\n",
    "        self.memory['state'].write(\n",
    "            step, \n",
    "            tf.constant(state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['next_state'].write(\n",
    "            step, \n",
    "            tf.constant(next_state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['rewards'].write(\n",
    "            step, \n",
    "            tf.constant(reward, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['done'].write(\n",
    "            step, \n",
    "            tf.constant(done, tf.int32)\n",
    "            ).mark_used()\n",
    "\n",
    "    def get_expected_return(\n",
    "        self,\n",
    "        rewards: tf.Tensor,\n",
    "        gamma: float,\n",
    "        standardize: bool = True):\n",
    "        \"\"\"Compute expected returns\"\"\"\n",
    "\n",
    "        n = tf.shape(rewards)[0] \n",
    "        returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "        # start at last reward and then accumulate reward sums into returns array\n",
    "        rewards = rewards[::-1]\n",
    "        discounted_sum = tf.constant(0.0)\n",
    "        discounted_sum_shape = discounted_sum.shape\n",
    "        for i in tf.range(n):\n",
    "            reward = rewards[i]\n",
    "            discounted_sum = reward + self.gamma * discounted_sum # discounted_sum= 0 for last reward (i.e. first element in loop)\n",
    "            discounted_sum.set_shape(discounted_sum_shape)\n",
    "            returns = returns.write(i, discounted_sum)\n",
    "        returns = returns.stack()[::-1] # take final element\n",
    "\n",
    "        if standardize:\n",
    "            returns = ((returns - tf.reduce_mean(returns)) / \n",
    "                        (tf.math.reduce_std(returns) + eps))\n",
    "        \n",
    "        return tf.expand_dims(returns, 1)\n",
    "\n",
    "    def update(self, episode: int):\n",
    "\n",
    "        states = tf.expand_dims(self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())]), 1)\n",
    "        rewards = self.memory['rewards'].gather([i for i in tf.range(self.memory['rewards'].size())])\n",
    "        returns = self.get_expected_return(rewards = rewards, gamma = self.gamma)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # action_probs, values = self.model(states)\n",
    "            action, action_probs, values = self._get_action(state)\n",
    "            # calculate the loss values\n",
    "            loss = self.compute_loss(action_probs[0, action], values, returns, episode)\n",
    "\n",
    "        # compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "        # apply gradients to model params\n",
    "        self.model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # episode_reward = tf.reduce_sum(rewards)\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step = episode)\n",
    "            tf.summary.scalar('episode_reward', tf.reduce_sum(rewards), step = episode)\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        action_probs: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        returns: tf.Tensor,\n",
    "        episode: int\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Computes combined actor-critic loss\"\"\"\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        action_log_probs = tf.math.log(action_probs)\n",
    "        actor_loss = -tf.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "        critic_loss = self.loss(values, returns)\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('actor_loss', actor_loss, step = episode)\n",
    "            tf.summary.scalar('critic_loss', critic_loss, step = episode)\n",
    "\n",
    "        return actor_loss + critic_loss\n",
    "\n",
    "def get_next_run(log_dir):\n",
    "    next_run = max([0]+[int(j) for j in [i.split('_')[-1] for i in os.listdir(log_dir)] if j.isdigit()]) + 1\n",
    "    return log_dir + f'/run_{next_run}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## create tensorboard logs\n",
    "LOGS = './logs'\n",
    "if not os.path.exists(LOGS):\n",
    "    os.mkdir(LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(j) for j in [i.split('_')[-1] for i in os.listdir(LOGS)] if j.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/run_5'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_run(LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to:  ./logs/run_5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "num_actions = env.action_space.n #.shape[0] for continuous\n",
    "num_hidden_units = 128\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "log_dir = get_next_run(LOGS) #os.path.join(LOGS, f'test/')\n",
    "print('Saving logs to: ', log_dir)\n",
    "summary_writer = tf.summary.create_file_writer(logdir = log_dir)\n",
    "agent = Agent(0.99, model, optimizer, summary_writer)\n",
    "\n",
    "NUM_EPISODES=200\n",
    "\n",
    "for i in range(NUM_EPISODES):\n",
    "    state, _ = env.reset(seed = seed)\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, step)\n",
    "        next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "        done = terminated or truncated\n",
    "        agent.log(state, next_state, reward, done, step)\n",
    "\n",
    "        step +=1\n",
    "\n",
    "    agent.update(i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
