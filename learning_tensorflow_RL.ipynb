{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL in tensorflow\n",
    "Some experiments to learn how to use tensorflow to do RL.\n",
    "\n",
    "We start with this demo: https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic\n",
    "\n",
    "Also tried this guy: https://github.com/philtabor/Actor-Critic-Methods-Paper-To-Code/blob/master/ActorCritic/tf2/agent.py \n",
    "\n",
    "This example was useful too for calculating the loss: https://blog.tensorflow.org/2018/07/deep-reinforcement-learning-keras-eager-execution.html\n",
    "\n",
    "Ultimately it seemed that using tfp.distributions.Categorical might have been slightly less stable than the blog above? But it seems that a key mistake that I made here was not using the _actual_ actions. Instead I was re-sampling from the probability distribution. This made a big difference. the softmax_cross_entropy_with_logits_v2 seems to help a lot.\n",
    "\n",
    "Appears to be quite unstable some times - can just collapse into using a single action and achieve very bad performance. Not sure why.\n",
    "\n",
    "Another troublesome occurrence is when the actor loss and the critic loss tend to diverge, with the actor loss growing faster than the critic loss. The critic loss looks to try to catch the actor loss. Results in the total loss to diverge.\n",
    "\n",
    "Have tried to avoid this by setting entropy regularisation high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.critic1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        # self.actor1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.common = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.actor = layers.Dense(num_actions,\n",
    "        #  activation = 'softmax'\n",
    "        )\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        # c = self.critic1(inputs)\n",
    "        # a = self.actor1(inputs)\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        gamma: float, \n",
    "        entropy_coef: float,\n",
    "        model: tf.keras.Model, \n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        summary_writer):\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.entropy_coef = tf.constant(entropy_coef)\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # creates a dictionary of tensor arrays to write to\n",
    "        self.memory = self._init_memory()\n",
    "        # self.loss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer)\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "    def _init_memory(self):\n",
    "        return {\n",
    "                'action' : tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True),\n",
    "                'state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'next_state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'action_probs': tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'values' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'rewards' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'done' : tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        }\n",
    "\n",
    "    \n",
    "    def _get_action(self, state):\n",
    "\n",
    "        # state = tf.expand_dims(tf.constant(state, tf.float32), 0)\n",
    "        action_logits, value = self.model(\n",
    "                tf.expand_dims(tf.constant(state, tf.float32), 0)\n",
    "            )\n",
    "        action_probs = tfp.distributions.Categorical(logits=action_logits)\n",
    "        action = action_probs.sample()[0]\n",
    "        # action_probs = tf.nn.softmax(action_logits)\n",
    "        # action = np.random.choice(2, p=action_probs.numpy()[0])\n",
    "\n",
    "        return action, action_probs, value\n",
    "    \n",
    "    def get_action(self, state, step):\n",
    "\n",
    "        # run the model\n",
    "        action, _, _ = self._get_action(state)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def log(self, action, state, next_state, reward, done, step):\n",
    "\n",
    "        \"\"\"\n",
    "        Logs results into memory - not all used necessarily\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory['action'].write(\n",
    "            step, \n",
    "            tf.constant(action, tf.int32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['state'].write(\n",
    "            step, \n",
    "            tf.constant(state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['next_state'].write(\n",
    "            step, \n",
    "            tf.constant(next_state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['rewards'].write(\n",
    "            step, \n",
    "            tf.constant(reward, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['done'].write(\n",
    "            step, \n",
    "            tf.constant(done, tf.int32)\n",
    "            ).mark_used()\n",
    "\n",
    "    def get_expected_return(\n",
    "        self,\n",
    "        rewards: tf.Tensor,\n",
    "        standardize: bool = True):\n",
    "        \"\"\"Compute expected returns\"\"\"\n",
    "\n",
    "        n = tf.shape(rewards)[0] \n",
    "        returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "        # start at last reward and then accumulate reward sums into returns array\n",
    "        rewards = rewards[::-1]\n",
    "        discounted_sum = tf.constant(0.0)\n",
    "        discounted_sum_shape = discounted_sum.shape\n",
    "        for i in tf.range(n):\n",
    "            reward = rewards[i]\n",
    "            discounted_sum = reward + self.gamma * discounted_sum # discounted_sum= 0 for last reward (i.e. first element in loop)\n",
    "            discounted_sum.set_shape(discounted_sum_shape)\n",
    "            returns = returns.write(i, discounted_sum)\n",
    "        returns = returns.stack()[::-1] # reverse order back to original\n",
    "\n",
    "        if standardize:\n",
    "            returns = ((returns - tf.reduce_mean(returns)) / \n",
    "                        (tf.math.reduce_std(returns) + eps))\n",
    "        \n",
    "        return returns #tf.expand_dims(returns, 1)\n",
    "\n",
    "    def update(self, episode: int):\n",
    "\n",
    "        states = tf.expand_dims(self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())]), 1)\n",
    "        rewards = self.memory['rewards'].gather([i for i in tf.range(self.memory['rewards'].size())])\n",
    "        actions = self.memory['action'].gather([i for i in tf.range(self.memory['action'].size())])\n",
    "        # tf.one_hot(\n",
    "        #     self.memory['action'].gather([i for i in tf.range(self.memory['action'].size())]),\n",
    "        #     2,\n",
    "        #     dtype = tf.int32\n",
    "        # )\n",
    "        returns = self.get_expected_return(rewards = rewards, standardize=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            action_logits, values = self.model(states)\n",
    "\n",
    "            # calculate the loss values\n",
    "            loss = self.compute_loss(\n",
    "                actions, \n",
    "                action_logits, \n",
    "                values,\n",
    "                returns,\n",
    "                episode)\n",
    "\n",
    "        # compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        # grads = [tf.clip_by_norm(g, 1) for g in grads]\n",
    "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step = episode)\n",
    "            tf.summary.scalar('episode_reward', tf.reduce_sum(rewards), step = episode)\n",
    "\n",
    "        # wipe memory for next episode\n",
    "        self.memory = self._init_memory()\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        actions: tf.Tensor,\n",
    "        action_logits: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        returns: tf.Tensor,\n",
    "        episode: int\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Computes combined actor-critic loss\"\"\"\n",
    "\n",
    "        advantage = returns - tf.squeeze(values)\n",
    "\n",
    "        critic_loss = tf.math.square(advantage)\n",
    "        # critic_loss = self.loss(values, returns)\n",
    "        action_probs = tfp.distributions.Categorical(logits=action_logits)\n",
    "\n",
    "        # action_probs = tf.squeeze(tf.nn.softmax(action_logits))\n",
    "\n",
    "        entropy_loss = -self.entropy_coef * action_probs.entropy()\n",
    "        #tf.reduce_sum(action_probs * tf.math.log(action_probs + eps), axis = 1)\n",
    "\n",
    "        actor_loss = -action_probs.log_prob(actions) * advantage\n",
    "        #tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels=actions, logits = tf.squeeze(action_logits)) * advantage\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('actor_loss', tf.reduce_mean(actor_loss), step = episode)\n",
    "            tf.summary.scalar('critic_loss', tf.reduce_mean(critic_loss), step = episode)\n",
    "            tf.summary.scalar('entropy_loss', tf.reduce_mean(entropy_loss), step = episode)\n",
    "\n",
    "        return tf.reduce_mean(actor_loss + critic_loss + entropy_loss)\n",
    "\n",
    "def get_next_run(log_dir):\n",
    "    next_run = max([0]+[int(j) for j in [i.split('_')[-1] for i in os.listdir(log_dir)] if j.isdigit()]) + 1\n",
    "    return log_dir + f'/run_{next_run}'\n",
    "\n",
    "def train(agent, env, num_episodes, seed):\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset(seed = seed)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state, step)\n",
    "            next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "            done = terminated or truncated\n",
    "            agent.log(action, state, next_state, reward, done, step)\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "        agent.update(i)\n",
    "\n",
    "def test(agent, env, num_episodes, seed):\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset(seed = seed)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state, step)\n",
    "            next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## create tensorboard logs\n",
    "LOGS = './logs'\n",
    "if not os.path.exists(LOGS):\n",
    "    os.mkdir(LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to:  ./logs/run_102\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "num_actions = env.action_space.n #.shape[0] for continuous\n",
    "num_hidden_units = 256\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "log_dir = get_next_run(LOGS) \n",
    "print('Saving logs to: ', log_dir)\n",
    "summary_writer = tf.summary.create_file_writer(logdir = log_dir)\n",
    "agent = Agent(0.99, 0.5, model, optimizer, summary_writer)\n",
    "\n",
    "NUM_EPISODES=300\n",
    "train(agent, env, NUM_EPISODES, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "test(agent, env, 10, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, _ = env.reset()\n",
    "state = np.random.rand(10,1,4)\n",
    "\n",
    "action, action_probs, value = agent._get_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.11920291, 0.880797  ], dtype=float32)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = tfp.distributions.Categorical(logits= tf.constant([1.,3.]))\n",
    "dist.prob(tf.constant([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.11920291 0.880797  ], shape=(2,), dtype=float32)\n",
      "tf.Tensor(0.36533388, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p1 = tf.nn.softmax(tf.constant([1., 3.]))\n",
    "print(p1)\n",
    "print(tf.reduce_sum(-tf.math.log(p1) * p1, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.36533386>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 1, 2), dtype=float32, numpy=\n",
       "array([[[[ -603.4104  ,   603.86115 ]],\n",
       "\n",
       "        [[-2137.5083  ,  2136.7122  ]],\n",
       "\n",
       "        [[-1716.2794  ,  1715.7451  ]],\n",
       "\n",
       "        [[ -940.38794 ,   940.67737 ]],\n",
       "\n",
       "        [[ -401.54602 ,   402.21472 ]],\n",
       "\n",
       "        [[-1815.1519  ,  1814.3895  ]],\n",
       "\n",
       "        [[  -80.182304,    81.40192 ]],\n",
       "\n",
       "        [[-1866.38    ,  1865.4476  ]],\n",
       "\n",
       "        [[-1512.7196  ,  1511.8994  ]],\n",
       "\n",
       "        [[-2159.2698  ,  2158.3687  ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs1 = tf.nn.softmax(action_probs.logits)\n",
    "probs2 = action_probs.prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 1, 2), dtype=float32, numpy=\n",
       "array([[[[0., 1.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 1), dtype=float32, numpy=\n",
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
