{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActor(tf.keras.Model):\n",
    "    \"\"\"Actor that outputs a policy directly\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int,\n",
    "        std_init = -2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.means = layers.Dense(num_actions)\n",
    "        # self.stds = tf.Variable(tf.ones(num_actions))*std_init#lambda x: tf.zeros(num_actions)\n",
    "        # self.stds = layers.Dense(num_actions, activation='relu')\n",
    "        # tf.ones(num_actions) * 0.25\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        means = self.means(inputs)\n",
    "        # stds = self.stds(inputs)\n",
    "        # stds = tf.clip_by_value(stds, 1.0e-3, 1)\n",
    "        return means\n",
    "        # return tfp.distributions.MultivariateNormalDiag(loc = means, scale_diag = tf.exp(self.stds))\n",
    "\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.common = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        # outputs scale, location params for mvn\n",
    "        self.actor = ContinuousActor(num_actions, num_hidden_units) #layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "class sepActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.actor1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        # outputs scale, location params for mvn\n",
    "        self.actor2 = ContinuousActor(num_actions, num_hidden_units) \n",
    "        self.critic1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.critic2 = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        a = self.actor1(inputs)\n",
    "        c = self.critic1(inputs)\n",
    "        return self.actor2(a), self.critic2(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions,\n",
    "        num_hidden_units\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.fc1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.fc2 = layers.Dense(num_actions)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hidden_units\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.fc1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.fc2 = layers.Dense(1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "alist = [1,2,3,4]\n",
    "atensor = tf.convert_to_tensor(alist)\n",
    "print(atensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        gamma,\n",
    "        lam,\n",
    "        ent_coef,\n",
    "        vf_coef,\n",
    "        clip,\n",
    "        timesteps_per_batch,\n",
    "        max_timesteps_per_episode,\n",
    "        n_updates_per_iteration,\n",
    "        actor,\n",
    "        optimizer_actor,\n",
    "        critic,\n",
    "        optimizer_critic,\n",
    "        summary_writer\n",
    "        ):\n",
    "\n",
    "        # env\n",
    "        self.env_name = env\n",
    "        self.env = gym.make(self.env_name)\n",
    "        \n",
    "        # learning params\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.clip = clip\n",
    "\n",
    "        # rollout params\n",
    "        self.timesteps_per_batch = timesteps_per_batch\n",
    "        self.max_timesteps_per_episode = max_timesteps_per_episode\n",
    "        self.n_updates_per_iteration = n_updates_per_iteration\n",
    "\n",
    "        # model \n",
    "        self.actor = actor\n",
    "        self.optimizer_actor = optimizer_actor\n",
    "        self.critic = critic\n",
    "        self.optimizer_critic = optimizer_critic\n",
    "\n",
    "        # other params\n",
    "        # TODO: generalise dimensions\n",
    "        self.stds = tf.constant([0.5, 0.5])\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        t_so_far = 0\n",
    "        while t_so_far < total_timesteps:\n",
    "            batch_states, batch_actions, batch_log_probs, batch_returns, batch_lens = self.rollout()\n",
    "\n",
    "            V, _ = self.evaluate(batch_states, batch_actions)\n",
    "\n",
    "            A_k = batch_returns - V\n",
    "\n",
    "            # normalise advantages\n",
    "            A_k = (A_k - tf.reduce_mean(A_k)) / (tf.math.reduce_std(A_k) + 1.0e-10)\n",
    "\n",
    "            ## set up loss records\n",
    "            running_actor_loss = 0\n",
    "            running_critic_loss = 0\n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    V, curr_log_probs = self.evaluate(batch_states, batch_actions)\n",
    "                    # this is the start of our computation graph?\n",
    "                    ratios = tf.exp(curr_log_probs - tf.squeeze(batch_log_probs))\n",
    "\n",
    "                    # calc surrogate losses\n",
    "                    surr1 = ratios * A_k\n",
    "                    surr2 = tf.clip_by_value(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "\n",
    "                    # get actor / critic losses \n",
    "                    actor_loss = tf.reduce_mean(-tf.math.minimum(surr1, surr2))\n",
    "                    critic_loss = tf.reduce_mean((V - batch_returns)**2)\n",
    "\n",
    "                # backprop actor loss\n",
    "                actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "                self.optimizer_actor.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "\n",
    "                # backprop critic loss\n",
    "                critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "                self.optimizer_critic.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "                running_actor_loss += tf.stop_gradient(actor_loss)\n",
    "                running_critic_loss += tf.stop_gradient(critic_loss)            \n",
    "            \n",
    "            t_so_far += np.sum(batch_lens)\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('actor_loss', running_actor_loss / self.n_updates_per_iteration, step = t_so_far)\n",
    "                tf.summary.scalar('critic_loss', running_critic_loss / self.n_updates_per_iteration, step = t_so_far)\n",
    "                tf.summary.scalar('returns', tf.reduce_mean(batch_returns), step = t_so_far)\n",
    "\n",
    "            \n",
    "            # with self.summary_writer.as_default():\n",
    "            #     tf.summary.scalar('total_reward', tf.reduce_mean(actor_loss), step = t_so_far)\n",
    "\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        Runs a single episode in the environment and accumulates data\n",
    "        \"\"\"\n",
    "\n",
    "        # store results\n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_log_probs = []\n",
    "        batch_rewards = []\n",
    "        batch_returns = []\n",
    "        batch_lens = []\n",
    "\n",
    "        # num timesteps so far\n",
    "        t = 0\n",
    "\n",
    "        while t <  self.timesteps_per_batch:\n",
    "            ep_rewards = []\n",
    "\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "\n",
    "                # increment timesteps\n",
    "                t += 1\n",
    "\n",
    "                # collect observations\n",
    "                batch_states.append(state)\n",
    "                \n",
    "                action, log_prob = self.get_action(state)\n",
    "                state, reward, truncated, terminated, _ = self.env.step(action)\n",
    "                done = truncated or terminated\n",
    "\n",
    "                # collect reward, action and log prob\n",
    "                ep_rewards.append(reward)\n",
    "                batch_actions.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # collect episode length and rewards\n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rewards.append(ep_rewards)\n",
    "\n",
    "        # convert to tensors\n",
    "        batch_states = tf.convert_to_tensor(batch_states, dtype = tf.float32)\n",
    "        batch_actions = tf.convert_to_tensor(batch_actions, dtype = tf.float32)\n",
    "        batch_log_probs = tf.convert_to_tensor(batch_log_probs, dtype = tf.float32)\n",
    "\n",
    "        batch_returns = self.compute_returns(batch_rewards)\n",
    "\n",
    "        return batch_states, batch_actions, batch_log_probs, batch_returns, batch_lens\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "\n",
    "        means = self.actor(tf.expand_dims(state, 0))\n",
    "        policy = tfp.distributions.MultivariateNormalDiag(loc = means, scale_diag = self.stds)\n",
    "\n",
    "        # TODO: squash / scale action\n",
    "        action = policy.sample()[0]\n",
    "        log_prob = policy.log_prob(action)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        batch_returns = []\n",
    "        for ep_rewards in reversed((rewards)):\n",
    "\n",
    "            discounted_reward = 0 # reward so far\n",
    "\n",
    "            for reward in reversed(ep_rewards):\n",
    "                discounted_reward = reward + self.gamma * discounted_reward\n",
    "                batch_returns.insert(0, discounted_reward)\n",
    "            \n",
    "        # convert to tensor\n",
    "        batch_returns = tf.convert_to_tensor(batch_returns, dtype = tf.float32)\n",
    "\n",
    "        return batch_returns\n",
    "\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        batch_advantages = []\n",
    "\n",
    "        for ep_rewards, ep_values, ep_dones in zip(rewards, values, dones):\n",
    "            advantages = []\n",
    "            last_advantage = 0\n",
    "\n",
    "            for t in reversed(range(len(ep_rewards))):\n",
    "                if t + 1 < len(ep_rewards):\n",
    "                    delta = ep_rewards[t] + self.gamma * ep_values[t+1] * (1 - ep_dones[t+1]) - ep_values[t]\n",
    "                else:\n",
    "                    delta = ep_rewards[t] - ep_values[t]\n",
    "\n",
    "                advantage = delta + self.gamma * self.lam * (1 - ep_dones[t]) * last_advantage\n",
    "                last_advantage = advantage\n",
    "                advantages.insert(0, advantage)\n",
    "\n",
    "            batch_advantages.extend(advantages)\n",
    "        \n",
    "        return tf.convert_to_tensor(batch_advantages, dtype = tf.float32)\n",
    "    \n",
    "    def evaluate(self, batch_states, batch_actions):\n",
    "        V = tf.squeeze(self.critic(batch_states))\n",
    "        mean = self.actor(batch_states)\n",
    "        policy = tfp.distributions.MultivariateNormalDiag(mean, self.stds)\n",
    "\n",
    "        log_probs = policy.log_prob(batch_actions)\n",
    "\n",
    "        return V, log_probs\n",
    "\n",
    "    def test(self, num_episodes):\n",
    "        env = gym.make(self.env_name, render_mode = 'human')\n",
    "        for i in range(num_episodes):\n",
    "            state, _ = env.reset(seed = seed)\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done:\n",
    "                action, _ = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                state = next_state\n",
    "\n",
    "                step += 1\n",
    "        env.close()\n",
    "\n",
    "def get_next_run(log_dir):\n",
    "    next_run = max([0]+[int(j) for j in [i.split('_')[-1] for i in os.listdir(log_dir)] if j.isdigit()]) + 1\n",
    "    return log_dir + f'/run_{next_run}'\n",
    "\n",
    "\n",
    "            # if standardize:\n",
    "            # advantages = ((advantages - tf.reduce_mean(advantages)) /\n",
    "            #                 (tf.math.reduce_std(advantages) + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to:  ./logs/run_195\n"
     ]
    }
   ],
   "source": [
    "ENV = 'LunarLanderContinuous-v2'#'Pendulum-v1'#\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env = gym.make(ENV)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "\n",
    "optimizer_actor = tf.keras.optimizers.RMSprop(learning_rate=0.0007, clipnorm=0.5)\n",
    "optimizer_critic = tf.keras.optimizers.RMSprop(learning_rate=0.0007, clipnorm=0.5)\n",
    "num_actions = env.action_space.shape[0]\n",
    "num_hidden_units = 256\n",
    "\n",
    "actor = Actor(num_actions, num_hidden_units)\n",
    "critic = Critic(num_hidden_units)\n",
    "\n",
    "\n",
    "log_dir = get_next_run('./logs') \n",
    "print('Saving logs to: ', log_dir)\n",
    "summary_writer = tf.summary.create_file_writer(logdir = log_dir)\n",
    "\n",
    "\n",
    "agent = PPO(        \n",
    "    env = ENV,\n",
    "    gamma = 0.99,\n",
    "    lam = 0.9,\n",
    "    ent_coef = 0.01,\n",
    "    vf_coef = 0.4,\n",
    "    clip = 0.2,\n",
    "    timesteps_per_batch = 10,\n",
    "    max_timesteps_per_episode = 20,\n",
    "    n_updates_per_iteration = 1,\n",
    "    actor = actor,\n",
    "    optimizer_actor = optimizer_actor,\n",
    "    critic = critic,\n",
    "    optimizer_critic = optimizer_critic,\n",
    "    summary_writer= summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV, render_mode='human')\n",
    "for i in range(30):\n",
    "    state, _ = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "property 'render_mode' of 'TimeLimit' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\61417\\Documents\\Units\\thesis_preparation\\right_left_brain_rl\\experiment\\working\\learn_tensorflow\\learning_tensorflow_RL_PPO.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/61417/Documents/Units/thesis_preparation/right_left_brain_rl/experiment/working/learn_tensorflow/learning_tensorflow_RL_PPO.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/61417/Documents/Units/thesis_preparation/right_left_brain_rl/experiment/working/learn_tensorflow/learning_tensorflow_RL_PPO.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env\u001b[39m.\u001b[39mrender()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/61417/Documents/Units/thesis_preparation/right_left_brain_rl/experiment/working/learn_tensorflow/learning_tensorflow_RL_PPO.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m state, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n",
      "\u001b[1;31mAttributeError\u001b[0m: property 'render_mode' of 'TimeLimit' object has no setter"
     ]
    }
   ],
   "source": [
    "env.render_mode = 'human'\n",
    "env.render()\n",
    "state, _ = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_states, batch_actions, batch_log_probs, batch_returns, batch_lens = agent.rollout()\n",
    "\n",
    "### TODO: Where does the gradient tape start for tf? here?\n",
    "V, _ = agent.evaluate(batch_states, batch_actions)\n",
    "\n",
    "A_k = batch_returns - V\n",
    "\n",
    "# normalise advantages\n",
    "A_k = (A_k - tf.reduce_mean(A_k)) / (tf.math.reduce_std(A_k) + 1.0e-10)\n",
    "\n",
    "for _ in range(1):\n",
    "    _, curr_log_probs = agent.evaluate(batch_states, batch_actions)\n",
    "    # this is the start of our computation graph?\n",
    "    ratios = tf.exp(curr_log_probs - tf.squeeze(batch_log_probs))\n",
    "\n",
    "    # calc surrogate losses\n",
    "    surr1 = ratios * A_k\n",
    "    surr2 = tf.clip_by_value(ratios, 1 - agent.clip, 1 + agent.clip) * A_k\n",
    "\n",
    "    # actor_loss = tf.math.minimum(surr1, surr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-8.606018e-08>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(-tf.math.minimum(surr1, surr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(187, 1), dtype=float32, numpy=\n",
       "array([[-0.86020947],\n",
       "       [-0.8257072 ],\n",
       "       [-0.75084114],\n",
       "       [-0.75237584],\n",
       "       [-0.670789  ],\n",
       "       [-0.4941579 ],\n",
       "       [-1.2827153 ],\n",
       "       [-2.4005966 ],\n",
       "       [-2.038274  ],\n",
       "       [-2.50885   ],\n",
       "       [-1.0050769 ],\n",
       "       [-0.535179  ],\n",
       "       [-0.72144794],\n",
       "       [-0.9657695 ],\n",
       "       [-0.9282563 ],\n",
       "       [-1.4808846 ],\n",
       "       [-0.987303  ],\n",
       "       [-1.4309921 ],\n",
       "       [-0.50107646],\n",
       "       [-0.648643  ],\n",
       "       [-1.7586565 ],\n",
       "       [-0.65567756],\n",
       "       [-1.0893621 ],\n",
       "       [-0.591717  ],\n",
       "       [-1.2166104 ],\n",
       "       [-0.85562086],\n",
       "       [-0.5078547 ],\n",
       "       [-2.2736385 ],\n",
       "       [-0.7293763 ],\n",
       "       [-1.2845066 ],\n",
       "       [-1.0116034 ],\n",
       "       [-1.8786213 ],\n",
       "       [-1.1035981 ],\n",
       "       [-0.5052619 ],\n",
       "       [-0.5018351 ],\n",
       "       [-1.9937894 ],\n",
       "       [-1.3588228 ],\n",
       "       [-1.5594311 ],\n",
       "       [-2.0590606 ],\n",
       "       [-0.51939607],\n",
       "       [-0.6041256 ],\n",
       "       [-0.4598049 ],\n",
       "       [-0.93824697],\n",
       "       [-0.9650216 ],\n",
       "       [-0.9239924 ],\n",
       "       [-1.1415067 ],\n",
       "       [-3.0996027 ],\n",
       "       [-4.626237  ],\n",
       "       [-2.2372131 ],\n",
       "       [-1.0045481 ],\n",
       "       [-2.0482593 ],\n",
       "       [-2.6735044 ],\n",
       "       [-2.0148923 ],\n",
       "       [-1.087806  ],\n",
       "       [-0.45294452],\n",
       "       [-4.8888636 ],\n",
       "       [-0.7039385 ],\n",
       "       [-1.6974108 ],\n",
       "       [-2.4756994 ],\n",
       "       [-1.20365   ],\n",
       "       [-1.5313368 ],\n",
       "       [-2.5507245 ],\n",
       "       [-1.0931273 ],\n",
       "       [-1.230675  ],\n",
       "       [-0.594095  ],\n",
       "       [-1.4383214 ],\n",
       "       [-0.7018688 ],\n",
       "       [-0.7552109 ],\n",
       "       [-0.7717149 ],\n",
       "       [-1.0764551 ],\n",
       "       [-0.9452915 ],\n",
       "       [-0.75606346],\n",
       "       [-0.5711063 ],\n",
       "       [-0.6272385 ],\n",
       "       [-0.63442063],\n",
       "       [-0.94089293],\n",
       "       [-1.3356757 ],\n",
       "       [-0.5683949 ],\n",
       "       [-0.99770284],\n",
       "       [-0.5066618 ],\n",
       "       [-1.0267615 ],\n",
       "       [-0.94808674],\n",
       "       [-0.70022225],\n",
       "       [-0.554405  ],\n",
       "       [-1.1766539 ],\n",
       "       [-1.4741979 ],\n",
       "       [-1.8956683 ],\n",
       "       [-2.125637  ],\n",
       "       [-0.8788109 ],\n",
       "       [-1.7438991 ],\n",
       "       [-4.586524  ],\n",
       "       [-2.9910302 ],\n",
       "       [-1.3734715 ],\n",
       "       [-2.5133529 ],\n",
       "       [-1.2083321 ],\n",
       "       [-2.0250368 ],\n",
       "       [-2.9182763 ],\n",
       "       [-1.1245856 ],\n",
       "       [-1.4898047 ],\n",
       "       [-2.1559298 ],\n",
       "       [-3.1744432 ],\n",
       "       [-0.584216  ],\n",
       "       [-1.2879064 ],\n",
       "       [-1.3084378 ],\n",
       "       [-0.7292485 ],\n",
       "       [-1.2185442 ],\n",
       "       [-1.1516194 ],\n",
       "       [-5.036411  ],\n",
       "       [-0.59412575],\n",
       "       [-0.6553731 ],\n",
       "       [-1.5733845 ],\n",
       "       [-4.5744257 ],\n",
       "       [-3.610765  ],\n",
       "       [-3.4444928 ],\n",
       "       [-0.7694583 ],\n",
       "       [-1.8431296 ],\n",
       "       [-1.7092941 ],\n",
       "       [-1.2991569 ],\n",
       "       [-1.0970917 ],\n",
       "       [-1.9353681 ],\n",
       "       [-1.35251   ],\n",
       "       [-0.5558517 ],\n",
       "       [-1.1362772 ],\n",
       "       [-1.5099444 ],\n",
       "       [-1.5842752 ],\n",
       "       [-2.1688266 ],\n",
       "       [-0.70500803],\n",
       "       [-0.9304118 ],\n",
       "       [-1.8221242 ],\n",
       "       [-1.737217  ],\n",
       "       [-0.90754604],\n",
       "       [-0.608207  ],\n",
       "       [-0.7700474 ],\n",
       "       [-0.49332643],\n",
       "       [-2.087114  ],\n",
       "       [-0.57773805],\n",
       "       [-1.7338636 ],\n",
       "       [-1.3275537 ],\n",
       "       [-2.6813202 ],\n",
       "       [-0.52368927],\n",
       "       [-0.61586905],\n",
       "       [-3.2480736 ],\n",
       "       [-1.1906369 ],\n",
       "       [-1.3060689 ],\n",
       "       [-1.923749  ],\n",
       "       [-5.6585503 ],\n",
       "       [-0.8371682 ],\n",
       "       [-0.93859243],\n",
       "       [-1.0820372 ],\n",
       "       [-2.678316  ],\n",
       "       [-1.4692786 ],\n",
       "       [-1.6642475 ],\n",
       "       [-4.5710387 ],\n",
       "       [-0.67023516],\n",
       "       [-0.5661384 ],\n",
       "       [-1.557807  ],\n",
       "       [-0.62988997],\n",
       "       [-2.1966407 ],\n",
       "       [-2.240965  ],\n",
       "       [-0.70075583],\n",
       "       [-1.0211568 ],\n",
       "       [-1.3412483 ],\n",
       "       [-0.68059444],\n",
       "       [-0.62811255],\n",
       "       [-1.2297673 ],\n",
       "       [-0.9702923 ],\n",
       "       [-1.2530582 ],\n",
       "       [-0.94284105],\n",
       "       [-0.46679544],\n",
       "       [-0.7051642 ],\n",
       "       [-0.6317303 ],\n",
       "       [-1.1229091 ],\n",
       "       [-0.6237817 ],\n",
       "       [-0.45280504],\n",
       "       [-0.5164902 ],\n",
       "       [-0.693928  ],\n",
       "       [-0.97513247],\n",
       "       [-0.7688303 ],\n",
       "       [-3.453301  ],\n",
       "       [-2.811964  ],\n",
       "       [-0.7472286 ],\n",
       "       [-0.52419925],\n",
       "       [-0.7020297 ],\n",
       "       [-4.0359297 ],\n",
       "       [-0.7019379 ],\n",
       "       [-1.1629956 ],\n",
       "       [-1.85481   ]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([1.        , 1.0000002 , 0.99999976, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.0000002 , 1.        , 1.        ,\n",
       "       1.        , 1.0000005 , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.0000002 , 1.0000002 , 1.        ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env, \n",
    "        gamma: float, \n",
    "        lam: float,\n",
    "        entropy_coef: float,\n",
    "        vf_coef: float,\n",
    "        scale: float,\n",
    "        timesteps_per_batch: int,\n",
    "        max_timesteps_per_episode: int,\n",
    "        model: tf.keras.Model, \n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        summary_writer,\n",
    "        normalise_rewards = False):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.lam = tf.constant(lam)\n",
    "        self.entropy_coef = tf.constant(entropy_coef)\n",
    "        self.vf_coef = tf.constant(vf_coef)\n",
    "        self.scale = scale\n",
    "\n",
    "        # rollout params\n",
    "        self.timesteps_per_batch = timesteps_per_batch\n",
    "        self.max_timesteps_per_episode = max_timesteps_per_episode\n",
    "\n",
    "        # model setup\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # creates a dictionary of tensor arrays to write to\n",
    "        self.memory = self._init_memory()\n",
    "        self.loss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer)\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        # switch for analysis\n",
    "        self.DEBUG = debug\n",
    "        if self.DEBUG:\n",
    "            self.debug_val = 0\n",
    "\n",
    "        self.normalise_rewards = normalise_rewards\n",
    "\n",
    "    def _init_memory(self):\n",
    "        return {\n",
    "                'action' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'next_state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'action_probs': tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'values' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'rewards' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'done' : tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        }\n",
    "\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        t_so_far = 0\n",
    "\n",
    "        while t_so_far < total_timesteps:\n",
    "\n",
    "\n",
    "    def rollout(self):\n",
    "        ## for each rollout we want to collect\n",
    "        # states (observations), actions, log_probs, rewards, rewards-to-go, length\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_rtgs = []\n",
    "        batch_lens = []\n",
    "\n",
    "        while t <  self.timesteps_per_batch:\n",
    "            ep_rews = []\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                \n",
    "                t+=1\n",
    "\n",
    "                batch_obs.append(obs)\n",
    "\n",
    "                action, log_prob = self.get_action(obs)\n",
    "                obs, rew, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rews.append(ep_rews)\n",
    "\n",
    "    \n",
    "    def _get_action(self, state):\n",
    "\n",
    "        policy, value = self.model(\n",
    "                tf.expand_dims(tf.constant(state, tf.float32), 0)\n",
    "            )\n",
    "        action = policy.sample()[0]\n",
    "\n",
    "        return self.scale * tf.tanh(action), policy, value\n",
    "        # return tf.clip_by_value(action, -0.99, 0.99), policy, value\n",
    "\n",
    "    def _squash_correction(self, action, dist):\n",
    "        log_probs = dist.log_prob(action)\n",
    "        tf.math.loc(1 - tf.math.pow(action, 2) + eps)\n",
    "        #tf.reduce_sum(-log_probs - tf.squeeze(tf.math.log(1 - tf.math.pow(converted_actions, 2) + 1.0e-10)), axis=1)\n",
    "\n",
    "    \n",
    "    def get_action(self, state, step):\n",
    "\n",
    "        # run the model\n",
    "        action, policy, _ = self._get_action(state)\n",
    "        log_probs = self._squash_correction(action, policy)\n",
    "        \n",
    "\n",
    "        # if self.DEBUG:\n",
    "        #     with self.summary_writer.as_default():\n",
    "        #         tf.summary.scalar('mean1', policy.mean(), step = self.debug_val)\n",
    "        #         tf.summary.scalar('mean2', policy.mean(), step = self.debug_val)\n",
    "        #         # tf.summary.scalar('std1', policy.stddev()[0], step = self.debug_val)\n",
    "        #         # tf.summary.scalar('std2', policy.stddev()[1], step = self.debug_val)\n",
    "\n",
    "        #     self.debug_val += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def log(self, action, state, next_state, reward, done, step):\n",
    "\n",
    "        \"\"\"\n",
    "        Logs results into memory - not all used necessarily\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory['action'].write(\n",
    "            step, \n",
    "            tf.constant(action, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['state'].write(\n",
    "            step, \n",
    "            tf.constant(state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['next_state'].write(\n",
    "            step, \n",
    "            tf.constant(next_state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['rewards'].write(\n",
    "            step, \n",
    "            tf.constant(reward, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['done'].write(\n",
    "            step, \n",
    "            tf.constant(done, tf.int32)\n",
    "            ).mark_used()\n",
    "\n",
    "    def get_gae(self, rewards, values, dones, standardize):\n",
    "        \n",
    "        n = tf.shape(dones)[0] \n",
    "        advantages = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "        last_advantage = tf.constant(0.)\n",
    "\n",
    "        for t in reversed(tf.range(tf.shape(rewards))):\n",
    "            if t + 1 < tf.shape(rewards):\n",
    "                \n",
    "                delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t + 1]) - values[t]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                delta = rewards[t] - values[t]\n",
    "\n",
    "            \n",
    "            advantage = delta + self.gamma * self.lam * (1 - dones[t]) * last_advantage\n",
    "            advantages.write(t, advantage).mark_used()\n",
    "\n",
    "        advantages = advantages.stack()[::-1]\n",
    "        if standardize:\n",
    "            advantages = ((advantages - tf.reduce_mean(advantages)) /\n",
    "                            (tf.math.reduce_std(advantages) + eps))\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def get_expected_return(\n",
    "        self,\n",
    "        rewards: tf.Tensor,\n",
    "        standardize: bool = True):\n",
    "        \"\"\"Compute expected returns\"\"\"\n",
    "\n",
    "        ### could convert this to generalised advantage estimator\n",
    "        ### get 'done' flags and multiply discounted sum and gamma by (1-done)\n",
    "        ### also need value estimates\n",
    "        ### do some checking, but if you're careful, the done value could enable you to do this process across batches\n",
    "        ### means you could do multiple batches per update - this might help the algorithm learn\n",
    "        n = tf.shape(rewards)[0] \n",
    "        returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "        # start at last reward and then accumulate reward sums into returns array\n",
    "        rewards = rewards[::-1]\n",
    "        discounted_sum = tf.constant(0.0)\n",
    "        discounted_sum_shape = discounted_sum.shape\n",
    "        for i in tf.range(n):\n",
    "            reward = rewards[i]\n",
    "            discounted_sum = reward + self.gamma * discounted_sum # discounted_sum= 0 for last reward (i.e. first element in loop)\n",
    "            discounted_sum.set_shape(discounted_sum_shape)\n",
    "            returns = returns.write(i, discounted_sum)\n",
    "        returns = returns.stack()[::-1] # reverse order back to original\n",
    "\n",
    "        if standardize:\n",
    "            returns = ((returns - tf.reduce_mean(returns)) / \n",
    "                        (tf.math.reduce_std(returns) + eps))\n",
    "        \n",
    "        return returns \n",
    "\n",
    "    def update(self, episode: int):\n",
    "\n",
    "        states = self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())])\n",
    "        # tf.expand_dims(self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())]), 1)\n",
    "        rewards = self.memory['rewards'].gather([i for i in tf.range(self.memory['rewards'].size())])\n",
    "        actions = self.memory['action'].gather([i for i in tf.range(self.memory['action'].size())])\n",
    "        dones = tf.cast(self.memory['done'].gather([i for i in tf.range(self.memory['done'].size())]), tf.float32)\n",
    "        # returns = self.get_expected_return(rewards = rewards, standardize=self.normalise_rewards)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            policy, values = self.model(states)\n",
    "\n",
    "            advantages = self.get_gae(rewards, values, dones, self.normalise_rewards)\n",
    "\n",
    "            # calculate the loss values\n",
    "            loss = self.compute_loss(\n",
    "                actions, \n",
    "                policy, \n",
    "                advantages,\n",
    "                # values,\n",
    "                # returns,\n",
    "                episode)\n",
    "\n",
    "        # compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step = episode)\n",
    "            tf.summary.scalar('episode_reward', tf.reduce_sum(rewards), step = episode)\n",
    "\n",
    "        # wipe memory for next episode\n",
    "        self.memory = self._init_memory()\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        actions: tf.Tensor,\n",
    "        policy: tf.Tensor,\n",
    "        advantages: tf.Tensor,\n",
    "        # values: tf.Tensor,\n",
    "        # returns: tf.Tensor,\n",
    "        episode: int\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Computes combined actor-critic loss\"\"\"\n",
    "        # print(f'returns: {tf.shape(returns)}; values: {tf.shape(values)}')\n",
    "        # advantage = returns - tf.squeeze(values)\n",
    "\n",
    "        critic_loss = self.vf_coef*tf.pow(advantages, 2)#self.loss(tf.squeeze(values), returns)#tf.math.square(advantage)\n",
    "\n",
    "        entropy_loss = -self.entropy_coef * policy.entropy()\n",
    "\n",
    "        # back to original scale\n",
    "        converted_actions = actions / self.scale\n",
    "        # squashing correction\n",
    "        log_probs = tf.expand_dims(policy.log_prob(converted_actions), axis = 1)\n",
    "        # stopping gradient for advantage dramatically improves stability!\n",
    "        actor_loss = tf.reduce_sum(-log_probs - tf.squeeze(tf.math.log(1 - tf.math.pow(converted_actions, 2) + 1.0e-10)), axis=1) * tf.stop_gradient(advantages)\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('actor_loss', tf.reduce_mean(actor_loss), step = episode)\n",
    "            tf.summary.scalar('critic_loss', tf.reduce_mean(critic_loss), step = episode)\n",
    "            tf.summary.scalar('entropy_loss', tf.reduce_mean(entropy_loss), step = episode)\n",
    "\n",
    "        return tf.reduce_mean(actor_loss + critic_loss + entropy_loss)\n",
    "\n",
    "def get_next_run(log_dir):\n",
    "    next_run = max([0]+[int(j) for j in [i.split('_')[-1] for i in os.listdir(log_dir)] if j.isdigit()]) + 1\n",
    "    return log_dir + f'/run_{next_run}'\n",
    "\n",
    "# def train(agent, env, num_episodes, seed):\n",
    "    \n",
    "#     for i in range(num_episodes):\n",
    "#         state, _ = env.reset(seed = seed)\n",
    "#         done = False\n",
    "#         step = 0\n",
    "#         while not done:\n",
    "#             action = agent.get_action(state, step)\n",
    "#             next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "#             done = terminated or truncated\n",
    "#             agent.log(action, state, next_state, reward, done, step)\n",
    "#             state = next_state\n",
    "\n",
    "#             step += 1\n",
    "#         agent.update(i)\n",
    "\n",
    "# def test(agent, env, num_episodes, seed):\n",
    "\n",
    "#     for i in range(num_episodes):\n",
    "#         state, _ = env.reset(seed = seed)\n",
    "#         done = False\n",
    "#         step = 0\n",
    "#         while not done:\n",
    "#             action = agent.get_action(state, step)\n",
    "#             next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "#             done = terminated or truncated\n",
    "#             state = next_state\n",
    "\n",
    "#             step += 1\n",
    "#     env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
