{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "from tensorflow.python.framework.ops import get_gradient_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActor(tf.keras.Model):\n",
    "    \"\"\"Actor that outputs a policy directly\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int,\n",
    "        std_init = -2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.means = layers.Dense(num_actions)\n",
    "        self.stds = tf.Variable(tf.zeros(num_actions))#*std_init#lambda x: tf.zeros(num_actions)\n",
    "        # self.stds = layers.Dense(num_actions, activation='relu')\n",
    "        # tf.ones(num_actions) * 0.25\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        means = self.means(inputs)\n",
    "        # stds = self.stds(inputs)\n",
    "        # stds = tf.clip_by_value(stds, 1.0e-3, 1)\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc = means, scale_diag = tf.exp(self.stds))\n",
    "\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.common = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        # outputs scale, location params for mvn\n",
    "        self.actor = ContinuousActor(num_actions, num_hidden_units) #layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "class sepActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.actor1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        # outputs scale, location params for mvn\n",
    "        self.actor2 = ContinuousActor(num_actions, num_hidden_units) \n",
    "        self.critic1 = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        self.critic2 = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        a = self.actor1(inputs)\n",
    "        c = self.critic1(inputs)\n",
    "        return self.actor2(a), self.critic2(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        gamma: float, \n",
    "        lam: float,\n",
    "        entropy_coef: float,\n",
    "        vf_coef: float,\n",
    "        scale: float,\n",
    "        model: tf.keras.Model, \n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        summary_writer,\n",
    "        debug = False,\n",
    "        normalise_rewards = False):\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.lam = tf.constant(lam)\n",
    "        self.entropy_coef = tf.constant(entropy_coef)\n",
    "        self.vf_coef = tf.constant(vf_coef)\n",
    "        self.scale = scale\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # creates a dictionary of tensor arrays to write to\n",
    "        self.memory = self._init_memory()\n",
    "        self.loss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer)\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        # switch for analysis\n",
    "        self.DEBUG = debug\n",
    "        if self.DEBUG:\n",
    "            self.debug_val = 0\n",
    "\n",
    "        self.normalise_rewards = normalise_rewards\n",
    "\n",
    "    def _init_memory(self):\n",
    "        return {\n",
    "                'action' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'next_state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'action_probs': tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'values' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'rewards' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'done' : tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        }\n",
    "\n",
    "    \n",
    "    def _get_action(self, state):\n",
    "\n",
    "        policy, value = self.model(\n",
    "                tf.expand_dims(tf.constant(state, tf.float32), 0)\n",
    "            )\n",
    "        action = policy.sample()[0]\n",
    "\n",
    "        return self.scale * tf.tanh(action), policy, value\n",
    "        # return tf.clip_by_value(action, -0.99, 0.99), policy, value\n",
    "    \n",
    "    def get_action(self, state, step):\n",
    "\n",
    "        # run the model\n",
    "        action, policy, _ = self._get_action(state)\n",
    "\n",
    "        if self.DEBUG:\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('mean1', policy.mean()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('mean2', policy.mean()[0][1], step = self.debug_val)\n",
    "                tf.summary.scalar('std1', policy.stddev()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('std2', policy.stddev()[0][1], step = self.debug_val)\n",
    "\n",
    "            self.debug_val += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def log(self, action, state, next_state, reward, done, step):\n",
    "\n",
    "        \"\"\"\n",
    "        Logs results into memory - not all used necessarily\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory['action'].write(\n",
    "            step, \n",
    "            tf.constant(action, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['state'].write(\n",
    "            step, \n",
    "            tf.constant(state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['next_state'].write(\n",
    "            step, \n",
    "            tf.constant(next_state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['rewards'].write(\n",
    "            step, \n",
    "            tf.constant(reward, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['done'].write(\n",
    "            step, \n",
    "            tf.constant(done, tf.int32)\n",
    "            ).mark_used()\n",
    "\n",
    "    def get_gae(self, rewards, values, dones, standardize):\n",
    "        \n",
    "        n = tf.shape(dones)[0] \n",
    "        advantages = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "        last_advantage = tf.constant(0.)\n",
    "\n",
    "        for t in reversed(tf.range(tf.shape(rewards))):\n",
    "            if t + 1 < tf.shape(rewards):\n",
    "                \n",
    "                delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t + 1]) - values[t]\n",
    "\n",
    "            else:\n",
    "                \n",
    "                delta = rewards[t] - values[t]\n",
    "\n",
    "            \n",
    "            advantage = delta + self.gamma * self.lam * (1 - dones[t]) * last_advantage\n",
    "            advantages.write(t, advantage).mark_used()\n",
    "\n",
    "        advantages = advantages.stack()[::-1]\n",
    "        if standardize:\n",
    "            advantages = ((advantages - tf.reduce_mean(advantages)) /\n",
    "                            (tf.math.reduce_std(advantages) + eps))\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def get_expected_return(\n",
    "        self,\n",
    "        rewards: tf.Tensor,\n",
    "        standardize: bool = True):\n",
    "        \"\"\"Compute expected returns\"\"\"\n",
    "\n",
    "        ### could convert this to generalised advantage estimator\n",
    "        ### get 'done' flags and multiply discounted sum and gamma by (1-done)\n",
    "        ### also need value estimates\n",
    "        ### do some checking, but if you're careful, the done value could enable you to do this process across batches\n",
    "        ### means you could do multiple batches per update - this might help the algorithm learn\n",
    "        n = tf.shape(rewards)[0] \n",
    "        returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "        # start at last reward and then accumulate reward sums into returns array\n",
    "        rewards = rewards[::-1]\n",
    "        discounted_sum = tf.constant(0.0)\n",
    "        discounted_sum_shape = discounted_sum.shape\n",
    "        for i in tf.range(n):\n",
    "            reward = rewards[i]\n",
    "            discounted_sum = reward + self.gamma * discounted_sum # discounted_sum= 0 for last reward (i.e. first element in loop)\n",
    "            discounted_sum.set_shape(discounted_sum_shape)\n",
    "            returns = returns.write(i, discounted_sum)\n",
    "        returns = returns.stack()[::-1] # reverse order back to original\n",
    "\n",
    "        if standardize:\n",
    "            returns = ((returns - tf.reduce_mean(returns)) / \n",
    "                        (tf.math.reduce_std(returns) + eps))\n",
    "        \n",
    "        return returns \n",
    "\n",
    "    def update(self, episode: int):\n",
    "\n",
    "        states = self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())])\n",
    "        # tf.expand_dims(self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())]), 1)\n",
    "        rewards = self.memory['rewards'].gather([i for i in tf.range(self.memory['rewards'].size())])\n",
    "        actions = self.memory['action'].gather([i for i in tf.range(self.memory['action'].size())])\n",
    "        dones = tf.cast(self.memory['done'].gather([i for i in tf.range(self.memory['done'].size())]), tf.float32)\n",
    "        # returns = self.get_expected_return(rewards = rewards, standardize=self.normalise_rewards)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            policy, values = self.model(states)\n",
    "\n",
    "            # adding values makes this into 'returns' and is optimised against the predicted value (i.e. output from model)\n",
    "            # value to add should be saved from rollout\n",
    "            advantages = self.get_gae(rewards, values, dones, self.normalise_rewards) #+ values\n",
    "\n",
    "            # calculate the loss values\n",
    "            loss = self.compute_loss(\n",
    "                actions, \n",
    "                policy, \n",
    "                advantages,\n",
    "                # values,\n",
    "                # returns,\n",
    "                episode)\n",
    "\n",
    "        # compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step = episode)\n",
    "            tf.summary.scalar('episode_reward', tf.reduce_sum(rewards), step = episode)\n",
    "\n",
    "        # wipe memory for next episode\n",
    "        self.memory = self._init_memory()\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        actions: tf.Tensor,\n",
    "        policy: tf.Tensor,\n",
    "        advantages: tf.Tensor,\n",
    "        # values: tf.Tensor,\n",
    "        # returns: tf.Tensor,\n",
    "        episode: int\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Computes combined actor-critic loss\"\"\"\n",
    "        # print(f'returns: {tf.shape(returns)}; values: {tf.shape(values)}')\n",
    "        # advantage = returns - tf.squeeze(values)\n",
    "\n",
    "        critic_loss = tf.pow(advantages, 2, name='critic_loss')#self.loss(tf.squeeze(values), returns)#tf.math.square(advantage)\n",
    "        # print(tf.reduce_mean(critic_loss))\n",
    "        # print(tf.reduce_mean(advantages))\n",
    "\n",
    "        entropy_loss = -policy.entropy()\n",
    "\n",
    "        # back to original scale\n",
    "        converted_actions = actions / self.scale\n",
    "        # squashing correction\n",
    "        log_probs = tf.expand_dims(policy.log_prob(converted_actions), axis = 1)\n",
    "        # stopping gradient for advantage dramatically improves stability!\n",
    "        actor_loss = -tf.reduce_sum(log_probs - tf.squeeze(tf.math.log(1 - tf.math.pow(converted_actions, 2) + 1.0e-10)), axis=1) * tf.stop_gradient(advantages)\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('actor_loss', tf.reduce_mean(actor_loss), step = episode)\n",
    "            tf.summary.scalar('critic_loss', tf.reduce_mean(critic_loss), step = episode)\n",
    "            tf.summary.scalar('entropy_loss', tf.reduce_mean(entropy_loss), step = episode)\n",
    "\n",
    "        # critic_grad_check = tf.get_default_graph().get_operation_by_name('critic_loss')\n",
    "        # print(get_gradient_function(critic_loss))\n",
    "        return tf.reduce_mean(actor_loss + self.vf_coef*critic_loss + self.entropy_coef *entropy_loss)\n",
    "\n",
    "def get_next_run(log_dir):\n",
    "    next_run = max([0]+[int(j) for j in [i.split('_')[-1] for i in os.listdir(log_dir)] if j.isdigit()]) + 1\n",
    "    return log_dir + f'/run_{next_run}'\n",
    "\n",
    "def train(agent, env, num_episodes, seed):\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset(seed = seed)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state, step)\n",
    "            next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "            done = terminated or truncated\n",
    "            agent.log(action, state, next_state, reward, done, step)\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "        agent.update(i)\n",
    "\n",
    "def test(agent, env, num_episodes, seed):\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset(seed = seed)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state, step)\n",
    "            next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create tensorboard logs\n",
    "LOGS = './logs'\n",
    "if not os.path.exists(LOGS):\n",
    "    os.mkdir(LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to:  ./logs/run_189\n"
     ]
    }
   ],
   "source": [
    "ENV = 'LunarLanderContinuous-v2'#'Pendulum-v1'#\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env = gym.make(ENV)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0007, clipnorm=0.5)\n",
    "num_actions = env.action_space.shape[0]\n",
    "num_hidden_units = 256\n",
    "# model = ActorCritic(num_actions, num_hidden_units)\n",
    "model = sepActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "log_dir = get_next_run(LOGS) \n",
    "print('Saving logs to: ', log_dir)\n",
    "summary_writer = tf.summary.create_file_writer(logdir = log_dir)\n",
    "agent = Agent(0.99,0.9, 0.0, 0.4, 1, model, optimizer, summary_writer, debug=False, normalise_rewards = True)\n",
    "\n",
    "NUM_EPISODES=100\n",
    "train(agent, env, NUM_EPISODES, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    state, _ = env.reset(seed = seed)\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(state, step)\n",
    "        next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "        done = terminated or truncated\n",
    "        agent.log(action, state, next_state, reward, done, step)\n",
    "        state = next_state\n",
    "        step +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = agent.memory['rewards'].gather([i for i in tf.range(agent.memory['rewards'].size())])\n",
    "states = agent.memory['state'].gather([i for i in tf.range(agent.memory['state'].size())])\n",
    "dones = agent.memory['done'].gather([i for i in tf.range(agent.memory['done'].size())])\n",
    "\n",
    "_, values = agent.model(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages = agent.get_gae(rewards, values, tf.cast(dones, tf.float32), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(164, 1), dtype=float32, numpy=\n",
       "array([[1.49467209e+02],\n",
       "       [6.89568281e+00],\n",
       "       [1.84197411e-01],\n",
       "       [5.71572363e-01],\n",
       "       [2.62087435e-02],\n",
       "       [6.23522811e-02],\n",
       "       [4.74430025e-02],\n",
       "       [1.76851332e-01],\n",
       "       [5.11911884e-02],\n",
       "       [4.70971828e-03],\n",
       "       [9.79589224e-02],\n",
       "       [2.93170698e-02],\n",
       "       [6.84715062e-03],\n",
       "       [3.07761040e-02],\n",
       "       [2.10136399e-02],\n",
       "       [1.26643879e-02],\n",
       "       [7.88297712e-06],\n",
       "       [1.16279072e-04],\n",
       "       [1.08048451e-04],\n",
       "       [2.68539414e-03],\n",
       "       [1.25033213e-04],\n",
       "       [1.50103704e-04],\n",
       "       [1.15281509e-04],\n",
       "       [8.81589111e-03],\n",
       "       [7.94908870e-03],\n",
       "       [2.95914686e-03],\n",
       "       [5.48998360e-03],\n",
       "       [4.22677733e-02],\n",
       "       [7.22334860e-03],\n",
       "       [1.62651972e-03],\n",
       "       [4.95722564e-03],\n",
       "       [3.92644026e-04],\n",
       "       [1.34479925e-02],\n",
       "       [6.74373889e-03],\n",
       "       [1.74059942e-02],\n",
       "       [2.38062363e-04],\n",
       "       [4.24834026e-04],\n",
       "       [2.39905179e-03],\n",
       "       [1.78332794e-02],\n",
       "       [2.20973347e-03],\n",
       "       [3.26597155e-03],\n",
       "       [4.96761873e-02],\n",
       "       [1.14672782e-03],\n",
       "       [1.53675253e-04],\n",
       "       [1.30356557e-03],\n",
       "       [2.59058288e-04],\n",
       "       [3.66125116e-03],\n",
       "       [1.02838520e-02],\n",
       "       [3.21918842e-03],\n",
       "       [2.11479445e-03],\n",
       "       [1.25479316e-02],\n",
       "       [3.32422592e-02],\n",
       "       [1.41209783e-03],\n",
       "       [7.66811967e-02],\n",
       "       [9.26460624e-02],\n",
       "       [2.16267303e-01],\n",
       "       [1.87797509e-02],\n",
       "       [3.79600358e-04],\n",
       "       [1.18081188e-02],\n",
       "       [4.99310568e-02],\n",
       "       [8.75111446e-02],\n",
       "       [5.76944556e-03],\n",
       "       [3.25795636e-02],\n",
       "       [2.57732160e-03],\n",
       "       [1.11571047e-03],\n",
       "       [3.93016785e-02],\n",
       "       [3.57126333e-02],\n",
       "       [2.00908282e-03],\n",
       "       [3.23171890e-03],\n",
       "       [1.08947689e-02],\n",
       "       [2.67586950e-02],\n",
       "       [5.17511666e-02],\n",
       "       [4.26155701e-03],\n",
       "       [4.87429043e-03],\n",
       "       [1.18417842e-02],\n",
       "       [3.82665265e-03],\n",
       "       [1.61938936e-01],\n",
       "       [2.24426445e-02],\n",
       "       [3.51490751e-02],\n",
       "       [5.87812662e-02],\n",
       "       [7.47176483e-02],\n",
       "       [4.13402803e-02],\n",
       "       [2.97773987e-01],\n",
       "       [8.37391019e-02],\n",
       "       [2.89815366e-02],\n",
       "       [3.72103080e-02],\n",
       "       [3.87803949e-02],\n",
       "       [1.93754174e-02],\n",
       "       [4.67248112e-02],\n",
       "       [5.33797033e-02],\n",
       "       [4.22062760e-04],\n",
       "       [5.25696985e-02],\n",
       "       [1.53164715e-01],\n",
       "       [4.11800221e-02],\n",
       "       [3.73878293e-02],\n",
       "       [1.22927362e-03],\n",
       "       [2.35468805e-01],\n",
       "       [6.81293458e-02],\n",
       "       [6.77404553e-02],\n",
       "       [3.02145593e-02],\n",
       "       [3.47252548e-01],\n",
       "       [3.50046493e-02],\n",
       "       [1.43928945e-01],\n",
       "       [2.53968060e-01],\n",
       "       [1.69292465e-02],\n",
       "       [1.56987190e-01],\n",
       "       [1.25431381e-02],\n",
       "       [1.56971309e-02],\n",
       "       [1.55431675e-02],\n",
       "       [3.57519998e-03],\n",
       "       [1.21025145e-02],\n",
       "       [1.31803349e-01],\n",
       "       [1.98940542e-02],\n",
       "       [1.39546283e-02],\n",
       "       [2.58380722e-04],\n",
       "       [7.63497129e-03],\n",
       "       [2.54732035e-02],\n",
       "       [8.14928953e-03],\n",
       "       [6.48621179e-04],\n",
       "       [7.94169009e-02],\n",
       "       [2.78880186e-02],\n",
       "       [3.39294709e-02],\n",
       "       [9.10492428e-03],\n",
       "       [6.09571021e-03],\n",
       "       [1.28386915e-03],\n",
       "       [4.88602323e-04],\n",
       "       [5.27163828e-03],\n",
       "       [8.31514131e-03],\n",
       "       [2.84220837e-02],\n",
       "       [1.66892428e-02],\n",
       "       [6.24912903e-02],\n",
       "       [6.18095882e-02],\n",
       "       [2.39612674e-03],\n",
       "       [6.02100790e-02],\n",
       "       [4.07098718e-02],\n",
       "       [6.72801491e-03],\n",
       "       [5.86514138e-02],\n",
       "       [7.02923611e-02],\n",
       "       [2.85722962e-07],\n",
       "       [4.36241459e-03],\n",
       "       [1.21769473e-01],\n",
       "       [4.31588776e-02],\n",
       "       [8.59729052e-02],\n",
       "       [2.47001972e-05],\n",
       "       [6.03136867e-02],\n",
       "       [3.62671502e-02],\n",
       "       [1.34655565e-01],\n",
       "       [6.32221848e-02],\n",
       "       [1.37573332e-01],\n",
       "       [1.35896027e-01],\n",
       "       [1.02812622e-03],\n",
       "       [4.46699150e-02],\n",
       "       [3.80326164e-05],\n",
       "       [3.96191850e-02],\n",
       "       [3.31588381e-05],\n",
       "       [1.37809485e-01],\n",
       "       [7.42397532e-02],\n",
       "       [9.81349051e-02],\n",
       "       [9.84857827e-02],\n",
       "       [2.23324052e-03],\n",
       "       [1.42368764e-01],\n",
       "       [1.20531302e-02],\n",
       "       [1.56180054e-01],\n",
       "       [1.36013448e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save_weights('./logs/run_154/model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.91900327e-05,  8.16159559e-03, -5.73932661e-03,  7.46810543e-04,\n",
       "        8.02413463e-05,  1.56790813e-03,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium.wrappers import NormalizeObservation\n",
    "env = NormalizeObservation(env)\n",
    "state, _ = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES=3\n",
    "env = gym.make(ENV, render_mode='human')\n",
    "test(agent, env, NUM_EPISODES, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN / sequence model\n",
    "Try to create an RL agent with an RNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActor(tf.keras.Model):\n",
    "    \"\"\"Actor that outputs a policy directly\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.means = layers.Dense(num_actions)\n",
    "        self.stds = lambda x: tf.zeros(num_actions)\n",
    "        # self.stds = layers.Dense(num_actions, activation='relu')\n",
    "        # tf.ones(num_actions) * 0.25\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        means = self.means(inputs)\n",
    "        stds = self.stds(inputs)\n",
    "        # stds = tf.clip_by_value(stds, 1.0e-3, 1)\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc = means, scale_diag = tf.exp(stds))\n",
    "\n",
    "\n",
    "class RecurrentActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.common = layers.LSTM(num_hidden_units)\n",
    "        # outputs scale, location params for mvn\n",
    "        self.actor = ContinuousActor(num_actions, num_hidden_units) \n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentAgent():\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        gamma: float, \n",
    "        entropy_coef: float,\n",
    "        vf_coef: float,\n",
    "        window_size: int,\n",
    "        scale: float,\n",
    "        model: tf.keras.Model, \n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        summary_writer,\n",
    "        debug = False,\n",
    "        normalise_rewards = False):\n",
    "        \n",
    "        # discount rate\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.entropy_coef = tf.constant(entropy_coef)\n",
    "        self.vf_coef = tf.constant(vf_coef)\n",
    "        self.window_size = tf.constant(window_size)\n",
    "        self.scale = tf.constant(scale)\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # creates a dictionary of tensor arrays to write to\n",
    "        self.memory = self._init_memory()\n",
    "        self.loss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer)\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        # switch for analysis\n",
    "        self.DEBUG = debug\n",
    "        if self.DEBUG:\n",
    "            self.debug_val = 0\n",
    "\n",
    "        self.normalise_rewards = normalise_rewards\n",
    "\n",
    "    def _init_memory(self):\n",
    "        return {\n",
    "                'action' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'next_state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'action_probs': tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'values' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'rewards' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'done' : tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        }\n",
    "\n",
    "    def _pad_input(self, new_state):\n",
    "        \n",
    "        # get previous states and combine with new_state\n",
    "        if self.memory['state'].size() > 0:\n",
    "            last_n = self.memory['state'].size()\n",
    "            first_n = np.max([0, last_n - self.window_size], 0)\n",
    "            prev_states = self.memory['state'].gather([i for i in tf.range(first_n, last_n)])\n",
    "            combined_states = tf.concat([tf.expand_dims(new_state, 0), prev_states], axis =0)\n",
    "        else:\n",
    "            combined_states = tf.expand_dims(new_state, 0)\n",
    "\n",
    "        # get size of padding - 0 if we have enough\n",
    "        pad_size = np.max([0, self.window_size-tf.shape(combined_states)[0]], axis = 0)\n",
    "        \n",
    "        # get the padded vals\n",
    "        padded = tf.pad(combined_states, [[0,pad_size],[0,0]])\n",
    "        padded = tf.expand_dims(padded, 0)\n",
    "\n",
    "        return padded\n",
    "    \n",
    "    def _get_action(self, state):\n",
    "\n",
    "        padded = self._pad_input(state)\n",
    "\n",
    "        policy, value = self.model(padded)\n",
    "        action = policy.sample()[0]\n",
    "\n",
    "        return self.scale * tf.tanh(action), policy, value\n",
    "\n",
    "    \n",
    "    def get_action(self, state, step):\n",
    "\n",
    "        # run the model\n",
    "        action, policy, _ = self._get_action(state)\n",
    "\n",
    "        if self.DEBUG:\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('mean1', policy.mean()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('mean2', policy.mean()[0][1], step = self.debug_val)\n",
    "                tf.summary.scalar('std1', policy.stddev()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('std2', policy.stddev()[0][1], step = self.debug_val)\n",
    "\n",
    "            self.debug_val += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def log(self, action, state, next_state, reward, done, step):\n",
    "\n",
    "        \"\"\"\n",
    "        Logs results into memory - not all used necessarily\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory['action'].write(\n",
    "            step, \n",
    "            tf.constant(action, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['state'].write(\n",
    "            step, \n",
    "            tf.constant(state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['next_state'].write(\n",
    "            step, \n",
    "            tf.constant(next_state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['rewards'].write(\n",
    "            step, \n",
    "            tf.constant(reward, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['done'].write(\n",
    "            step, \n",
    "            tf.constant(done, tf.int32)\n",
    "            ).mark_used()\n",
    "\n",
    "    def get_expected_return(\n",
    "        self,\n",
    "        rewards: tf.Tensor,\n",
    "        standardize: bool = True):\n",
    "        \"\"\"Compute expected returns\"\"\"\n",
    "\n",
    "        n = tf.shape(rewards)[0] \n",
    "        returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "        # start at last reward and then accumulate reward sums into returns array\n",
    "        rewards = rewards[::-1]\n",
    "        discounted_sum = tf.constant(0.0)\n",
    "        discounted_sum_shape = discounted_sum.shape\n",
    "        for i in tf.range(n):\n",
    "            reward = rewards[i]\n",
    "            discounted_sum = reward + self.gamma * discounted_sum # discounted_sum= 0 for last reward (i.e. first element in loop)\n",
    "            discounted_sum.set_shape(discounted_sum_shape)\n",
    "            returns = returns.write(i, discounted_sum)\n",
    "        returns = returns.stack()[::-1] # reverse order back to original\n",
    "\n",
    "        if standardize:\n",
    "            returns = ((returns - tf.reduce_mean(returns)) / \n",
    "                        (tf.math.reduce_std(returns) + eps))\n",
    "        \n",
    "        return returns\n",
    "\n",
    "    def _prepare_states_for_lstm(self, states):\n",
    "        \"\"\"wraps states into lstm sequence format\"\"\"\n",
    "        ## need the initial k \n",
    "\n",
    "        initial_padded_states = tf.convert_to_tensor([tf.pad(states[:(i+1)][::-1], [[0,np.max([0, self.window_size-(i+1)], axis = 0)],[0,0]]) for i in tf.range(0, self.window_size)])\n",
    "        remaining_states = tf.convert_to_tensor([states[(i-self.window_size):i] for i in tf.range(self.window_size, tf.shape(states)[0])])\n",
    "        combined_states = tf.concat([initial_padded_states, remaining_states], axis = 0)\n",
    "\n",
    "        return combined_states\n",
    "        \n",
    "\n",
    "    def update(self, episode: int):\n",
    "\n",
    "        states = self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())])\n",
    "        states = self._prepare_states_for_lstm(states)\n",
    "        rewards = self.memory['rewards'].gather([i for i in tf.range(self.memory['rewards'].size())])\n",
    "        actions = self.memory['action'].gather([i for i in tf.range(self.memory['action'].size())])\n",
    "        returns = self.get_expected_return(rewards = rewards, standardize=self.normalise_rewards)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            policy, values = self.model(states)\n",
    "\n",
    "            # calculate the loss values\n",
    "            loss = self.compute_loss(\n",
    "                actions, \n",
    "                policy, \n",
    "                values,\n",
    "                returns,\n",
    "                episode)\n",
    "\n",
    "        # compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step = episode)\n",
    "            tf.summary.scalar('episode_reward', tf.reduce_sum(rewards), step = episode)\n",
    "\n",
    "        # wipe memory for next episode\n",
    "        self.memory = self._init_memory()\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        actions: tf.Tensor,\n",
    "        policy: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        returns: tf.Tensor,\n",
    "        episode: int\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Computes combined actor-critic loss\"\"\"\n",
    "        # print(f'returns: {tf.shape(returns)}; values: {tf.shape(values)}')\n",
    "        advantage = returns - tf.squeeze(values)\n",
    "\n",
    "        critic_loss = self.vf_coef*self.loss(tf.squeeze(values), returns)#tf.math.square(advantage)\n",
    "\n",
    "        entropy_loss = -self.entropy_coef * policy.entropy()\n",
    "\n",
    "        converted_actions = actions/self.scale\n",
    "        # squashing correction\n",
    "        log_probs = tf.expand_dims(policy.log_prob(converted_actions), axis = 1)\n",
    "        # stopping gradient for advantage dramatically improves stability!\n",
    "        actor_loss = tf.reduce_sum(-log_probs - tf.squeeze(tf.math.log(1 - tf.math.pow(converted_actions, 2) + 1.0e-10)), axis=1) * tf.stop_gradient(advantage)\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('actor_loss', tf.reduce_mean(actor_loss), step = episode)\n",
    "            tf.summary.scalar('critic_loss', tf.reduce_mean(critic_loss), step = episode)\n",
    "            tf.summary.scalar('entropy_loss', tf.reduce_mean(entropy_loss), step = episode)\n",
    "\n",
    "        return tf.reduce_mean(actor_loss + critic_loss + entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to:  ./logs/run_165\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0007, clipnorm=0.5)\n",
    "num_actions = env.action_space.shape[0]\n",
    "num_hidden_units = 256\n",
    "model = RecurrentActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "log_dir = get_next_run(LOGS) \n",
    "print('Saving logs to: ', log_dir)\n",
    "summary_writer = tf.summary.create_file_writer(logdir = log_dir)\n",
    "agent2 = RecurrentAgent(0.99, 0.0, 0.4, 8, 1., model, optimizer, summary_writer, debug=True, normalise_rewards=False)\n",
    "\n",
    "NUM_EPISODES=100\n",
    "train(agent2, env, NUM_EPISODES, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
