{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActor(tf.keras.Model):\n",
    "    \"\"\"Actor that outputs a policy directly\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.means = layers.Dense(num_actions, \n",
    "        # activation = 'tanh'\n",
    "        )\n",
    "        # self.stds = lambda x: tf.zeros(num_actions)\n",
    "        self.stds = layers.Dense(num_actions, activation='relu')\n",
    "        # tf.ones(num_actions) * 0.25\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        means = self.means(inputs)\n",
    "        stds = self.stds(inputs)\n",
    "        stds = tf.clip_by_value(stds, 1.0e-3, 1)\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc = means, scale_diag = tf.exp(stds))\n",
    "\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.common = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        # outputs scale, location params for mvn\n",
    "        self.actor = ContinuousActor(num_actions, num_hidden_units) #layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        gamma: float, \n",
    "        entropy_coef: float,\n",
    "        vf_coef: float,\n",
    "        model: tf.keras.Model, \n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        summary_writer,\n",
    "        debug = False):\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.entropy_coef = tf.constant(entropy_coef)\n",
    "        self.vf_coef = tf.constant(vf_coef)\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # creates a dictionary of tensor arrays to write to\n",
    "        self.memory = self._init_memory()\n",
    "        self.loss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer)\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        # switch for analysis\n",
    "        self.DEBUG = debug\n",
    "        if self.DEBUG:\n",
    "            self.debug_val = 0\n",
    "\n",
    "    def _init_memory(self):\n",
    "        return {\n",
    "                'action' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'next_state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'action_probs': tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'values' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'rewards' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'done' : tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        }\n",
    "\n",
    "    \n",
    "    def _get_action(self, state):\n",
    "\n",
    "        policy, value = self.model(\n",
    "                tf.expand_dims(tf.constant(state, tf.float32), 0)\n",
    "            )\n",
    "        action = policy.sample()[0]\n",
    "\n",
    "        return tf.tanh(action), policy, value\n",
    "        # return tf.clip_by_value(action, -0.99, 0.99), policy, value\n",
    "    \n",
    "    def get_action(self, state, step):\n",
    "\n",
    "        # run the model\n",
    "        action, policy, _ = self._get_action(state)\n",
    "\n",
    "        if self.DEBUG:\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('mean1', policy.mean()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('mean2', policy.mean()[0][1], step = self.debug_val)\n",
    "                tf.summary.scalar('std1', policy.stddev()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('std2', policy.stddev()[0][1], step = self.debug_val)\n",
    "\n",
    "            self.debug_val += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def log(self, action, state, next_state, reward, done, step):\n",
    "\n",
    "        \"\"\"\n",
    "        Logs results into memory - not all used necessarily\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory['action'].write(\n",
    "            step, \n",
    "            tf.constant(action, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['state'].write(\n",
    "            step, \n",
    "            tf.constant(state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['next_state'].write(\n",
    "            step, \n",
    "            tf.constant(next_state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['rewards'].write(\n",
    "            step, \n",
    "            tf.constant(reward, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['done'].write(\n",
    "            step, \n",
    "            tf.constant(done, tf.int32)\n",
    "            ).mark_used()\n",
    "\n",
    "    def get_expected_return(\n",
    "        self,\n",
    "        rewards: tf.Tensor,\n",
    "        standardize: bool = True):\n",
    "        \"\"\"Compute expected returns\"\"\"\n",
    "\n",
    "        n = tf.shape(rewards)[0] \n",
    "        returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "        # start at last reward and then accumulate reward sums into returns array\n",
    "        rewards = rewards[::-1]\n",
    "        discounted_sum = tf.constant(0.0)\n",
    "        discounted_sum_shape = discounted_sum.shape\n",
    "        for i in tf.range(n):\n",
    "            reward = rewards[i]\n",
    "            discounted_sum = reward + self.gamma * discounted_sum # discounted_sum= 0 for last reward (i.e. first element in loop)\n",
    "            discounted_sum.set_shape(discounted_sum_shape)\n",
    "            returns = returns.write(i, discounted_sum)\n",
    "        returns = returns.stack()[::-1] # reverse order back to original\n",
    "\n",
    "        if standardize:\n",
    "            returns = ((returns - tf.reduce_mean(returns)) / \n",
    "                        (tf.math.reduce_std(returns) + eps))\n",
    "        \n",
    "        return returns \n",
    "\n",
    "    def update(self, episode: int):\n",
    "\n",
    "        states = self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())])\n",
    "        # tf.expand_dims(self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())]), 1)\n",
    "        rewards = self.memory['rewards'].gather([i for i in tf.range(self.memory['rewards'].size())])\n",
    "        actions = self.memory['action'].gather([i for i in tf.range(self.memory['action'].size())])\n",
    "        returns = self.get_expected_return(rewards = rewards, standardize=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            policy, values = self.model(states)\n",
    "\n",
    "            # calculate the loss values\n",
    "            loss = self.compute_loss(\n",
    "                actions, \n",
    "                policy, \n",
    "                values,\n",
    "                returns,\n",
    "                episode)\n",
    "\n",
    "        # compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step = episode)\n",
    "            tf.summary.scalar('episode_reward', tf.reduce_sum(rewards), step = episode)\n",
    "\n",
    "        # wipe memory for next episode\n",
    "        self.memory = self._init_memory()\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        actions: tf.Tensor,\n",
    "        policy: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        returns: tf.Tensor,\n",
    "        episode: int\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Computes combined actor-critic loss\"\"\"\n",
    "        # print(f'returns: {tf.shape(returns)}; values: {tf.shape(values)}')\n",
    "        advantage = returns - tf.squeeze(values)\n",
    "\n",
    "        critic_loss = self.vf_coef*self.loss(tf.squeeze(values), returns)#tf.math.square(advantage)\n",
    "\n",
    "        entropy_loss = -self.entropy_coef * policy.entropy()\n",
    "\n",
    "        # squashing correction\n",
    "        log_probs = tf.expand_dims(policy.log_prob(actions), axis = 1)\n",
    "        # stopping gradient for advantage dramatically improves stability!\n",
    "        actor_loss = tf.reduce_sum(-log_probs - tf.squeeze(tf.math.log(1 - tf.math.pow(actions, 2) + 1.0e-10)), axis=1) * tf.stop_gradient(advantage)\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('actor_loss', tf.reduce_mean(actor_loss), step = episode)\n",
    "            tf.summary.scalar('critic_loss', tf.reduce_mean(critic_loss), step = episode)\n",
    "            tf.summary.scalar('entropy_loss', tf.reduce_mean(entropy_loss), step = episode)\n",
    "\n",
    "        return tf.reduce_mean(actor_loss + critic_loss + entropy_loss)\n",
    "\n",
    "def get_next_run(log_dir):\n",
    "    next_run = max([0]+[int(j) for j in [i.split('_')[-1] for i in os.listdir(log_dir)] if j.isdigit()]) + 1\n",
    "    return log_dir + f'/run_{next_run}'\n",
    "\n",
    "def train(agent, env, num_episodes, seed):\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset(seed = seed)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state, step)\n",
    "            next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "            done = terminated or truncated\n",
    "            agent.log(action, state, next_state, reward, done, step)\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "        agent.update(i)\n",
    "\n",
    "def test(agent, env, num_episodes, seed):\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset(seed = seed)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state, step)\n",
    "            next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create tensorboard logs\n",
    "LOGS = './logs'\n",
    "if not os.path.exists(LOGS):\n",
    "    os.mkdir(LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to:  ./logs/run_143\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0007, clipnorm=0.5)\n",
    "num_actions = env.action_space.shape[0]\n",
    "num_hidden_units = 256\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "log_dir = get_next_run(LOGS) \n",
    "print('Saving logs to: ', log_dir)\n",
    "summary_writer = tf.summary.create_file_writer(logdir = log_dir)\n",
    "agent = Agent(0.99, 0.0, 0.4, model, optimizer, summary_writer, debug=True)\n",
    "\n",
    "NUM_EPISODES=600\n",
    "train(agent, env, NUM_EPISODES, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.84584044e-05,  8.14826722e-03, -4.40632277e-03, -2.24742149e-03,\n",
       "        5.62189279e-05,  1.10504436e-03,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium.wrappers import NormalizeObservation\n",
    "env = NormalizeObservation(env)\n",
    "state, _ = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES=5\n",
    "env = gym.make('LunarLanderContinuous-v2', render_mode='human')\n",
    "test(agent, env, NUM_EPISODES, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN / sequence model\n",
    "Try to create an RL agent with an RNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.Sequential(\n",
    "    layers.LSTM(128)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActor(tf.keras.Model):\n",
    "    \"\"\"Actor that outputs a policy directly\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.means = layers.Dense(num_actions)\n",
    "        self.stds = lambda x: tf.zeros(num_actions)\n",
    "        # self.stds = layers.Dense(num_actions, activation='relu')\n",
    "        # tf.ones(num_actions) * 0.25\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        means = self.means(inputs)\n",
    "        stds = self.stds(inputs)\n",
    "        # stds = tf.clip_by_value(stds, 1.0e-3, 1)\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc = means, scale_diag = tf.exp(stds))\n",
    "\n",
    "\n",
    "class RecurrentActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.common = layers.LSTM(num_hidden_units)\n",
    "        # outputs scale, location params for mvn\n",
    "        self.actor = ContinuousActor(num_actions, num_hidden_units) \n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentAgent():\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        gamma: float, \n",
    "        entropy_coef: float,\n",
    "        vf_coef: float,\n",
    "        window_size: int,\n",
    "        model: tf.keras.Model, \n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        summary_writer,\n",
    "        debug = False):\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.entropy_coef = tf.constant(entropy_coef)\n",
    "        self.vf_coef = tf.constant(vf_coef)\n",
    "        self.window_size = tf.constant(window_size)\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # creates a dictionary of tensor arrays to write to\n",
    "        self.memory = self._init_memory()\n",
    "        self.loss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer)\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        # switch for analysis\n",
    "        self.DEBUG = debug\n",
    "        if self.DEBUG:\n",
    "            self.debug_val = 0\n",
    "\n",
    "    def _init_memory(self):\n",
    "        return {\n",
    "                'action' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'next_state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'action_probs': tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'values' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'rewards' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'done' : tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        }\n",
    "\n",
    "    def _pad_input(self, new_state):\n",
    "        \n",
    "        # get previous states and combine with new_state\n",
    "        if self.memory['state'].size() > 0:\n",
    "            last_n = self.memory['state'].size()\n",
    "            first_n = np.max([0, last_n - self.window_size], 0)\n",
    "            prev_states = self.memory['state'].gather([i for i in tf.range(first_n, last_n)])\n",
    "            combined_states = tf.concat([tf.expand_dims(new_state, 0), prev_states], axis =0)\n",
    "        else:\n",
    "            combined_states = tf.expand_dims(new_state, 0)\n",
    "\n",
    "        # get size of padding - 0 if we have enough\n",
    "        pad_size = np.max([0, self.window_size-tf.shape(combined_states)[0]], axis = 0)\n",
    "        \n",
    "        # get the padded vals\n",
    "        padded = tf.pad(combined_states, [[0,pad_size],[0,0]])\n",
    "        padded = tf.expand_dims(padded, 0)\n",
    "\n",
    "        return padded\n",
    "    \n",
    "    def _get_action(self, state):\n",
    "\n",
    "        padded = self._pad_input(state)\n",
    "\n",
    "        policy, value = self.model(padded)\n",
    "        action = policy.sample()[0]\n",
    "\n",
    "        return tf.tanh(action), policy, value\n",
    "        # return tf.clip_by_value(action, -0.99, 0.99), policy, value\n",
    "    \n",
    "    def get_action(self, state, step):\n",
    "\n",
    "        # run the model\n",
    "        action, policy, _ = self._get_action(state)\n",
    "\n",
    "        if self.DEBUG:\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('mean1', policy.mean()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('mean2', policy.mean()[0][1], step = self.debug_val)\n",
    "                tf.summary.scalar('std1', policy.stddev()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('std2', policy.stddev()[0][1], step = self.debug_val)\n",
    "\n",
    "            self.debug_val += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def log(self, action, state, next_state, reward, done, step):\n",
    "\n",
    "        \"\"\"\n",
    "        Logs results into memory - not all used necessarily\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory['action'].write(\n",
    "            step, \n",
    "            tf.constant(action, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['state'].write(\n",
    "            step, \n",
    "            tf.constant(state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['next_state'].write(\n",
    "            step, \n",
    "            tf.constant(next_state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['rewards'].write(\n",
    "            step, \n",
    "            tf.constant(reward, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['done'].write(\n",
    "            step, \n",
    "            tf.constant(done, tf.int32)\n",
    "            ).mark_used()\n",
    "\n",
    "    def get_expected_return(\n",
    "        self,\n",
    "        rewards: tf.Tensor,\n",
    "        standardize: bool = True):\n",
    "        \"\"\"Compute expected returns\"\"\"\n",
    "\n",
    "        n = tf.shape(rewards)[0] \n",
    "        returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "        # start at last reward and then accumulate reward sums into returns array\n",
    "        rewards = rewards[::-1]\n",
    "        discounted_sum = tf.constant(0.0)\n",
    "        discounted_sum_shape = discounted_sum.shape\n",
    "        for i in tf.range(n):\n",
    "            reward = rewards[i]\n",
    "            discounted_sum = reward + self.gamma * discounted_sum # discounted_sum= 0 for last reward (i.e. first element in loop)\n",
    "            discounted_sum.set_shape(discounted_sum_shape)\n",
    "            returns = returns.write(i, discounted_sum)\n",
    "        returns = returns.stack()[::-1] # reverse order back to original\n",
    "\n",
    "        if standardize:\n",
    "            returns = ((returns - tf.reduce_mean(returns)) / \n",
    "                        (tf.math.reduce_std(returns) + eps))\n",
    "        \n",
    "        return returns\n",
    "\n",
    "    def _prepare_states_for_lstm(self, states):\n",
    "        \"\"\"wraps states into lstm sequence format\"\"\"\n",
    "        ## need the initial k \n",
    "\n",
    "        initial_padded_states = tf.convert_to_tensor([tf.pad(states[:(i+1)][::-1], [[0,np.max([0, self.window_size-(i+1)], axis = 0)],[0,0]]) for i in tf.range(0, self.window_size)])\n",
    "        remaining_states = tf.convert_to_tensor([states[(i-self.window_size):i] for i in tf.range(self.window_size, tf.shape(states)[0])])\n",
    "        combined_states = tf.concat([initial_padded_states, remaining_states], axis = 0)\n",
    "\n",
    "        return combined_states\n",
    "        \n",
    "\n",
    "    def update(self, episode: int):\n",
    "\n",
    "        states = self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())])\n",
    "        states = self._prepare_states_for_lstm(states)\n",
    "        rewards = self.memory['rewards'].gather([i for i in tf.range(self.memory['rewards'].size())])\n",
    "        actions = self.memory['action'].gather([i for i in tf.range(self.memory['action'].size())])\n",
    "        returns = self.get_expected_return(rewards = rewards, standardize=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            policy, values = self.model(states)\n",
    "\n",
    "            # calculate the loss values\n",
    "            loss = self.compute_loss(\n",
    "                actions, \n",
    "                policy, \n",
    "                values,\n",
    "                returns,\n",
    "                episode)\n",
    "\n",
    "        # compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step = episode)\n",
    "            tf.summary.scalar('episode_reward', tf.reduce_sum(rewards), step = episode)\n",
    "\n",
    "        # wipe memory for next episode\n",
    "        self.memory = self._init_memory()\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        actions: tf.Tensor,\n",
    "        policy: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        returns: tf.Tensor,\n",
    "        episode: int\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Computes combined actor-critic loss\"\"\"\n",
    "        # print(f'returns: {tf.shape(returns)}; values: {tf.shape(values)}')\n",
    "        advantage = returns - tf.squeeze(values)\n",
    "\n",
    "        critic_loss = self.vf_coef*self.loss(tf.squeeze(values), returns)#tf.math.square(advantage)\n",
    "\n",
    "        entropy_loss = -self.entropy_coef * policy.entropy()\n",
    "\n",
    "        # squashing correction\n",
    "        log_probs = tf.expand_dims(policy.log_prob(actions), axis = 1)\n",
    "        # stopping gradient for advantage dramatically improves stability!\n",
    "        actor_loss = tf.reduce_sum(-log_probs - tf.squeeze(tf.math.log(1 - tf.math.pow(actions, 2) + 1.0e-10)), axis=1) * tf.stop_gradient(advantage)\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('actor_loss', tf.reduce_mean(actor_loss), step = episode)\n",
    "            tf.summary.scalar('critic_loss', tf.reduce_mean(critic_loss), step = episode)\n",
    "            tf.summary.scalar('entropy_loss', tf.reduce_mean(entropy_loss), step = episode)\n",
    "\n",
    "        return tf.reduce_mean(actor_loss + critic_loss + entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to:  ./logs/run_153\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0007, clipnorm=0.5)\n",
    "num_actions = env.action_space.shape[0]\n",
    "num_hidden_units = 256\n",
    "model = RecurrentActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "log_dir = get_next_run(LOGS) \n",
    "print('Saving logs to: ', log_dir)\n",
    "summary_writer = tf.summary.create_file_writer(logdir = log_dir)\n",
    "agent2 = RecurrentAgent(0.99, 0.0, 0.4, 8, model, optimizer, summary_writer, debug=True)\n",
    "\n",
    "NUM_EPISODES=100\n",
    "train(agent2, env, NUM_EPISODES, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
