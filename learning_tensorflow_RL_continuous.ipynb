{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActor(tf.keras.Model):\n",
    "    \"\"\"Actor that outputs a policy directly\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.means = layers.Dense(num_actions, \n",
    "        # activation = 'tanh'\n",
    "        )\n",
    "        # self.stds = lambda x: tf.zeros(num_actions)\n",
    "        self.stds = layers.Dense(num_actions, activation='relu')\n",
    "        # tf.ones(num_actions) * 0.25\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        means = self.means(inputs)\n",
    "        stds = self.stds(inputs)\n",
    "        stds = tf.clip_by_value(stds, 1.0e-3, 1)\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc = means, scale_diag = tf.exp(stds))\n",
    "\n",
    "\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"combined actor-critic network. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        num_hidden_units: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.common = layers.Dense(num_hidden_units, activation = 'relu')\n",
    "        # outputs scale, location params for mvn\n",
    "        self.actor = ContinuousActor(num_actions, num_hidden_units) #layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        gamma: float, \n",
    "        entropy_coef: float,\n",
    "        vf_coef: float,\n",
    "        model: tf.keras.Model, \n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        summary_writer,\n",
    "        debug = False):\n",
    "\n",
    "        # discount rate\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.entropy_coef = tf.constant(entropy_coef)\n",
    "        self.vf_coef = tf.constant(vf_coef)\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # creates a dictionary of tensor arrays to write to\n",
    "        self.memory = self._init_memory()\n",
    "        self.loss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer)\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        # switch for analysis\n",
    "        self.DEBUG = debug\n",
    "        if self.DEBUG:\n",
    "            self.debug_val = 0\n",
    "\n",
    "    def _init_memory(self):\n",
    "        return {\n",
    "                'action' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'next_state' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'action_probs': tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'values' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'rewards' : tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True),\n",
    "                'done' : tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        }\n",
    "\n",
    "    \n",
    "    def _get_action(self, state):\n",
    "\n",
    "        policy, value = self.model(\n",
    "                tf.expand_dims(tf.constant(state, tf.float32), 0)\n",
    "            )\n",
    "        action = policy.sample()[0]\n",
    "\n",
    "        return tf.tanh(action), policy, value\n",
    "        # return tf.clip_by_value(action, -0.99, 0.99), policy, value\n",
    "    \n",
    "    def get_action(self, state, step):\n",
    "\n",
    "        # run the model\n",
    "        action, policy, _ = self._get_action(state)\n",
    "\n",
    "        if self.DEBUG:\n",
    "            with self.summary_writer.as_default():\n",
    "                tf.summary.scalar('mean1', policy.mean()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('mean2', policy.mean()[0][1], step = self.debug_val)\n",
    "                tf.summary.scalar('std1', policy.stddev()[0][0], step = self.debug_val)\n",
    "                tf.summary.scalar('std2', policy.stddev()[0][1], step = self.debug_val)\n",
    "\n",
    "            self.debug_val += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def log(self, action, state, next_state, reward, done, step):\n",
    "\n",
    "        \"\"\"\n",
    "        Logs results into memory - not all used necessarily\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory['action'].write(\n",
    "            step, \n",
    "            tf.constant(action, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['state'].write(\n",
    "            step, \n",
    "            tf.constant(state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['next_state'].write(\n",
    "            step, \n",
    "            tf.constant(next_state, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['rewards'].write(\n",
    "            step, \n",
    "            tf.constant(reward, tf.float32)\n",
    "            ).mark_used()\n",
    "\n",
    "        self.memory['done'].write(\n",
    "            step, \n",
    "            tf.constant(done, tf.int32)\n",
    "            ).mark_used()\n",
    "\n",
    "    def get_expected_return(\n",
    "        self,\n",
    "        rewards: tf.Tensor,\n",
    "        standardize: bool = True):\n",
    "        \"\"\"Compute expected returns\"\"\"\n",
    "\n",
    "        n = tf.shape(rewards)[0] \n",
    "        returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "        # start at last reward and then accumulate reward sums into returns array\n",
    "        rewards = rewards[::-1]\n",
    "        discounted_sum = tf.constant(0.0)\n",
    "        discounted_sum_shape = discounted_sum.shape\n",
    "        for i in tf.range(n):\n",
    "            reward = rewards[i]\n",
    "            discounted_sum = reward + self.gamma * discounted_sum # discounted_sum= 0 for last reward (i.e. first element in loop)\n",
    "            discounted_sum.set_shape(discounted_sum_shape)\n",
    "            returns = returns.write(i, discounted_sum)\n",
    "        returns = returns.stack()[::-1] # reverse order back to original\n",
    "\n",
    "        if standardize:\n",
    "            returns = ((returns - tf.reduce_mean(returns)) / \n",
    "                        (tf.math.reduce_std(returns) + eps))\n",
    "        \n",
    "        return returns \n",
    "\n",
    "    def update(self, episode: int):\n",
    "\n",
    "        states = self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())])\n",
    "        # tf.expand_dims(self.memory['state'].gather([i for i in tf.range(self.memory['state'].size())]), 1)\n",
    "        rewards = self.memory['rewards'].gather([i for i in tf.range(self.memory['rewards'].size())])\n",
    "        actions = self.memory['action'].gather([i for i in tf.range(self.memory['action'].size())])\n",
    "        returns = self.get_expected_return(rewards = rewards, standardize=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            policy, values = self.model(states)\n",
    "\n",
    "            # calculate the loss values\n",
    "            loss = self.compute_loss(\n",
    "                actions, \n",
    "                policy, \n",
    "                values,\n",
    "                returns,\n",
    "                episode)\n",
    "\n",
    "        # compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss, step = episode)\n",
    "            tf.summary.scalar('episode_reward', tf.reduce_sum(rewards), step = episode)\n",
    "\n",
    "        # wipe memory for next episode\n",
    "        self.memory = self._init_memory()\n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        actions: tf.Tensor,\n",
    "        policy: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        returns: tf.Tensor,\n",
    "        episode: int\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Computes combined actor-critic loss\"\"\"\n",
    "        # print(f'returns: {tf.shape(returns)}; values: {tf.shape(values)}')\n",
    "        advantage = returns - tf.squeeze(values)\n",
    "\n",
    "        critic_loss = self.vf_coef*self.loss(tf.squeeze(values), returns)#tf.math.square(advantage)\n",
    "\n",
    "        entropy_loss = -self.entropy_coef * policy.entropy()\n",
    "\n",
    "        # squashing correction\n",
    "        log_probs = tf.expand_dims(policy.log_prob(actions), axis = 1)\n",
    "        # stopping gradient for advantage dramatically improves stability!\n",
    "        actor_loss = tf.reduce_sum(-log_probs - tf.squeeze(tf.math.log(1 - tf.math.pow(actions, 2) + 1.0e-10)), axis=1) * tf.stop_gradient(advantage)\n",
    "\n",
    "        with self.summary_writer.as_default():\n",
    "            tf.summary.scalar('actor_loss', tf.reduce_mean(actor_loss), step = episode)\n",
    "            tf.summary.scalar('critic_loss', tf.reduce_mean(critic_loss), step = episode)\n",
    "            tf.summary.scalar('entropy_loss', tf.reduce_mean(entropy_loss), step = episode)\n",
    "\n",
    "        return tf.reduce_mean(actor_loss + critic_loss + entropy_loss)\n",
    "\n",
    "def get_next_run(log_dir):\n",
    "    next_run = max([0]+[int(j) for j in [i.split('_')[-1] for i in os.listdir(log_dir)] if j.isdigit()]) + 1\n",
    "    return log_dir + f'/run_{next_run}'\n",
    "\n",
    "def train(agent, env, num_episodes, seed):\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset(seed = seed)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state, step)\n",
    "            next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "            done = terminated or truncated\n",
    "            agent.log(action, state, next_state, reward, done, step)\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "        agent.update(i)\n",
    "\n",
    "def test(agent, env, num_episodes, seed):\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state, _ = env.reset(seed = seed)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.get_action(state, step)\n",
    "            next_state, reward, terminated, truncated, info = env.step(np.array(action))\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create tensorboard logs\n",
    "LOGS = './logs'\n",
    "if not os.path.exists(LOGS):\n",
    "    os.mkdir(LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to:  ./logs/run_142\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0007, clipnorm=0.5)\n",
    "num_actions = env.action_space.shape[0]\n",
    "num_hidden_units = 256\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "log_dir = get_next_run(LOGS) \n",
    "print('Saving logs to: ', log_dir)\n",
    "summary_writer = tf.summary.create_file_writer(logdir = log_dir)\n",
    "agent = Agent(0.99, 0.0, 0.4, model, optimizer, summary_writer, debug=True)\n",
    "\n",
    "NUM_EPISODES=300\n",
    "train(agent, env, NUM_EPISODES, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.84584044e-05,  8.14826722e-03, -4.40632277e-03, -2.24742149e-03,\n",
       "        5.62189279e-05,  1.10504436e-03,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium.wrappers import NormalizeObservation\n",
    "env = NormalizeObservation(env)\n",
    "state, _ = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES=5\n",
    "env = gym.make('LunarLanderContinuous-v2', render_mode='human')\n",
    "test(agent, env, NUM_EPISODES, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
